{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_glove_file = r\"/Users/anshuman/UMass/CS 682/Project/glove.6B/glove.6b.300d.txt\" ## glove text file that you download from internet, 6b words 300d,\n",
    "new_glove_file =r\"glove6b300d.bin\"\n",
    "movie_data_file =r\"ml-1m/movies.dat\" #movie data file\n",
    "user_data_file =r\"ml-1m/users.dat\" #user data file\n",
    "rating_data_file = r\"ml-1m/ratings.dat\" #rating data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this only once ever, if you have skip this and go to the next cell\n",
    "## Insert your glove location below.\n",
    "## This is done to convert .txt glove to binary format for easier loading.\n",
    "glove_file = datapath(og_glove_file)\n",
    "tmp_file = get_tmpfile(\"glove.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "model.save_word2vec_format(new_glove_file,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshuman/anaconda3/envs/cs682/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    " ##load binary glove model\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format(new_glove_file,binary=True)\n",
    "##get vocabulary\n",
    "vocab = list(glove_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the dataset into dataframes\n",
    "\n",
    "movieData = pd.read_csv(movie_data_file,sep=\"::\",names=[\"MovieID\",\"Name\",\"Genres\"],engine='python')\n",
    "userData = pd.read_csv(user_data_file,sep=\"::\",names=[\"UserID\",\"Gender\",\"Age\",\"Occupation\",\"Zipcode\"],engine='python')\n",
    "ratingData = pd.read_csv(rating_data_file,sep=\"::\",names=[\"UserID\",\"MovieID\",\"Rating\",\"Timestamp\"],engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## movie data preprocessing  - getting a dictionary of movie name and id pairs,\n",
    "##creating genrelist, and reseting movieData dataframe\n",
    "\n",
    "movieNameData = movieData[\"Name\"].unique()\n",
    "movieNameDict = {movieNameData[i]:i for i in range(0,movieNameData.shape[0])}\n",
    "genreList = [\"Action\",\"Adventure\",\"Animation\",\"Children's\",\"Comedy\",\"Crime\",\"Documentary\",\"Drama\",\"Fantasy\",\"Film-Noir\",\n",
    "              \"Horror\",\"Musical\",\"Mystery\",\"Romance\",\"Sci-Fi\",\"Thriller\",\"War\",\"Western\"]\n",
    "genres = movieData[\"Genres\"].apply(lambda x : x.split(\"|\"))\n",
    "movieData.reset_index(inplace=True,drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 3)\n",
      "Movie Embeddings (3953, 318)\n"
     ]
    }
   ],
   "source": [
    "## Create movie data vector based on embeddings or without.\n",
    "## genre data is on hot encoded since each film could have one or more genre. Array size of the one hot encoding is 18 \n",
    "## structure : movieid:genredata for no  word2vec embedding\n",
    "## structure : movie_name_embedding:genredata for no  word2vec embedding\n",
    "\n",
    "### In MovieEncodingEmbedding(), case-sensitivity is removed and also all characters which are not alpha numeric are removed.\n",
    "### If the word isn't part of glove vocab, a 0 vector is used in its stead.\n",
    "# def MovieEncodingNoEmbedding(movieid):\n",
    "    \n",
    "#     movieDataIDRow = movieData.loc[movieData[\"MovieID\"]==movieid]\n",
    "#     movieDataMovieID = movieNameDict[movieDataIDRow.Name.values[0]]\n",
    "#     movieDataGenreEncoding = [1 if i in movieDataIDRow.Genres.values[0].split(\"|\") else 0  for i in genreList]\n",
    "#     data = [movieDataMovieID]+movieDataGenreEncoding\n",
    "#     return np.array(data)\n",
    "\n",
    "# def MovieEncodingEmbedding(movieid):\n",
    "#     movieDataIDRow = movieData.loc[movieData[\"MovieID\"]==movieid]\n",
    "# #     print(\"Movie name\", movieDataIDRow.Name)\n",
    "#     movieDataMovieID = ''.join([char for char in movieDataIDRow.Name.values[0].lower() if (char.isalpha() or char==\" \")]).split(\" \")\n",
    "#     movieNameEmbedding = np.mean([glove_model[word] if (word in vocab) else np.zeros((1,300)) for word in movieDataMovieID],axis=0)                            \n",
    "#     movieDataGenreEncoding = np.array([1 if i in movieDataIDRow.Genres.values[0].split(\"|\") else 0  for i in genreList])\n",
    "#     data = np.concatenate((movieNameEmbedding, movieDataGenreEncoding.reshape(1,movieDataGenreEncoding.shape[0])), axis=None)\n",
    "#     return data.reshape(1,data.shape[0])\n",
    "\n",
    "from data_preparation import MovieEncodingEmbedding\n",
    "\n",
    "print(movieData.shape)\n",
    "numRows = movieData.shape[0]\n",
    "# print(movieData, movieData.loc[0][0], movieData.loc[0][1], movieData.loc[0][2])\n",
    "movie_embeddings = np.zeros((3953, 318))\n",
    "for rowNum in range(numRows):\n",
    "    movieID = movieData.loc[rowNum]['MovieID']\n",
    "#     print(rowNum, movieID)\n",
    "    movie_embeddings[movieID] = MovieEncodingEmbedding(movieID, movieData, genreList, glove_model, vocab)\n",
    "\n",
    "print(\"Movie Embeddings\", movie_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "from data_preparation import UserSequences\n",
    "\n",
    "## get user Sequences\n",
    "userSequences = UserSequences(ratingData,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5421\n",
      "27\n",
      "(3873,)\n"
     ]
    }
   ],
   "source": [
    "print(len(userSequences))\n",
    "print(len(userSequences[0]))\n",
    "print(userSequences[99][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get user-movie encodings for the data with embedding flag false and write it to file.\n",
    "## In every file each line will contain\n",
    "##userid::gender::userDataAge::userDataOccupation::userDataZipCode::movieid::(one hot genre encoding of len 18[each separated by ::]):rating \n",
    "# data = UserMoviePairEncodingBatch(userSequences,False)\n",
    "\n",
    "# with open(\"usermovie.dat\",\"w\") as f:\n",
    "#     for i in range(len(data)):\n",
    "#         for j in range(data[i].shape[0]):\n",
    "#             f.write(\"::\".join(data[i][j]))\n",
    "#             f.write(\"\\n\")\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get user-movie encodings for the data with embedding flag false and write it to file.\n",
    "## In every file each line will contain\n",
    "##userid::gender::userDataAge::userDataOccupation::userDataZipCode::(movieid_embedding[len 300, separated by ::])::(one hot genre encoding of len 18[each separated by ::]):rating \n",
    "\n",
    "# data = UserMoviePairEncodingBatch(userSequences,True)\n",
    "\n",
    "# with open(\"usermoviembeddings.dat\",\"w\") as f:\n",
    "#     for i in range(len(data)):\n",
    "#         for j in range(data[i].shape[0]):\n",
    "#             f.write(\"::\".join(data[i][j]))\n",
    "#             f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3468, 27) (3468, 26, 318) (3468, 26)\n",
      "val (868, 27) (868, 26, 318) (868, 26)\n",
      "test (1085, 27) (1085, 26, 318) (1085, 26)\n"
     ]
    }
   ],
   "source": [
    "# np.array(data).shape\n",
    "# print (\"data\", len(data), data[0].shape)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def fix(data):\n",
    "    newData = np.empty(data.shape, dtype=int)\n",
    "    for i, userSeq in enumerate(data):\n",
    "        for j, tup in enumerate(userSeq):\n",
    "            newData[i][j] = tup[0]\n",
    "    return newData\n",
    "\n",
    "data = fix(np.array(userSequences))\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "train_inp = movie_embeddings[train_data[:, :-1]]\n",
    "train_tgt = train_data[:, 1:]\n",
    "\n",
    "test_inp = movie_embeddings[test_data[:, :-1]]\n",
    "test_tgt = test_data[:, 1:]\n",
    "\n",
    "val_inp = movie_embeddings[val_data[:, :-1]]\n",
    "val_tgt = val_data[:, 1:]\n",
    "\n",
    "print (\"train\", train_data.shape, train_inp.shape, train_tgt.shape)\n",
    "print (\"val\", val_data.shape, val_inp.shape, val_tgt.shape)\n",
    "print (\"test\", test_data.shape, test_inp.shape, test_tgt.shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train torch.Size([3468, 26, 1, 318]) torch.Size([3468, 26, 1])\n",
      "Val torch.Size([868, 26, 1, 318]) torch.Size([868, 26, 1])\n",
      "Test torch.Size([1085, 26, 1, 318]) torch.Size([1085, 26, 1])\n",
      "| epoch   1 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2098.57 | loss  7.27 | ppl  1436.76\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2125.62 | loss  7.02 | ppl  1122.87\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2152.05 | loss  6.93 | ppl  1018.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 74.56s | valid loss  6.84 | valid ppl   932.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |\n",
      "Learning Rate 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshuman/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/anshuman/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/anshuman/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2216.40 | loss  6.79 | ppl   887.00\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2208.44 | loss  6.63 | ppl   754.50\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2118.21 | loss  6.55 | ppl   699.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 76.22s | valid loss  6.54 | valid ppl   690.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2116.71 | loss  6.46 | ppl   639.42\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2158.68 | loss  6.33 | ppl   559.04\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2128.53 | loss  6.29 | ppl   539.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 74.92s | valid loss  6.42 | valid ppl   612.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2090.28 | loss  6.23 | ppl   510.06\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2098.08 | loss  6.12 | ppl   454.52\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2138.26 | loss  6.10 | ppl   447.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 73.93s | valid loss  6.34 | valid ppl   564.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2098.61 | loss  6.06 | ppl   428.70\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2111.82 | loss  5.96 | ppl   386.06\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2105.50 | loss  5.95 | ppl   382.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 73.95s | valid loss  6.28 | valid ppl   532.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2107.75 | loss  5.92 | ppl   371.45\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2086.31 | loss  5.82 | ppl   336.51\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2103.57 | loss  5.81 | ppl   334.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 73.75s | valid loss  6.25 | valid ppl   518.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2098.33 | loss  5.79 | ppl   328.26\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2100.45 | loss  5.70 | ppl   298.43\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2105.60 | loss  5.69 | ppl   296.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 73.66s | valid loss  6.24 | valid ppl   514.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2103.34 | loss  5.68 | ppl   293.91\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2100.19 | loss  5.59 | ppl   267.66\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2093.92 | loss  5.58 | ppl   265.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 73.64s | valid loss  6.23 | valid ppl   510.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2103.51 | loss  5.59 | ppl   266.69\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2101.12 | loss  5.50 | ppl   243.79\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2107.51 | loss  5.49 | ppl   241.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 73.84s | valid loss  6.26 | valid ppl   524.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2102.38 | loss  5.50 | ppl   243.98\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2096.56 | loss  5.41 | ppl   224.75\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2104.98 | loss  5.41 | ppl   222.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 73.89s | valid loss  6.28 | valid ppl   533.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2105.98 | loss  5.43 | ppl   227.32\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2124.40 | loss  5.35 | ppl   210.44\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2178.02 | loss  5.34 | ppl   209.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 74.77s | valid loss  6.31 | valid ppl   549.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2106.38 | loss  5.37 | ppl   213.86\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2099.55 | loss  5.29 | ppl   199.23\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2108.33 | loss  5.29 | ppl   198.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 73.85s | valid loss  6.31 | valid ppl   551.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2100.51 | loss  5.31 | ppl   202.84\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2092.65 | loss  5.24 | ppl   188.54\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2095.57 | loss  5.24 | ppl   188.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 73.55s | valid loss  6.32 | valid ppl   553.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2106.14 | loss  5.27 | ppl   193.64\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2098.75 | loss  5.19 | ppl   179.95\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2105.67 | loss  5.20 | ppl   181.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 73.85s | valid loss  6.34 | valid ppl   564.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2107.83 | loss  5.22 | ppl   185.49\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2107.38 | loss  5.15 | ppl   172.99\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2094.33 | loss  5.16 | ppl   173.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 73.83s | valid loss  6.33 | valid ppl   561.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2102.69 | loss  5.19 | ppl   179.16\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2100.85 | loss  5.11 | ppl   165.19\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2100.82 | loss  5.12 | ppl   167.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 73.92s | valid loss  6.34 | valid ppl   568.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2110.44 | loss  5.14 | ppl   170.89\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2106.19 | loss  5.08 | ppl   160.64\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2101.78 | loss  5.08 | ppl   161.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 74.27s | valid loss  6.35 | valid ppl   573.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |\n",
      "Learning Rate 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2107.28 | loss  5.11 | ppl   166.04\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2106.73 | loss  5.04 | ppl   154.41\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2095.57 | loss  5.05 | ppl   155.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 73.85s | valid loss  6.38 | valid ppl   591.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2136.47 | loss  5.08 | ppl   161.56\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2100.19 | loss  5.00 | ppl   148.96\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2098.22 | loss  5.02 | ppl   150.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 74.17s | valid loss  6.38 | valid ppl   591.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2103.89 | loss  5.05 | ppl   155.93\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2107.77 | loss  4.97 | ppl   144.42\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2100.75 | loss  4.98 | ppl   145.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 73.88s | valid loss  6.42 | valid ppl   616.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2112.32 | loss  5.02 | ppl   151.51\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2102.30 | loss  4.95 | ppl   140.98\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2109.23 | loss  4.96 | ppl   142.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 74.03s | valid loss  6.44 | valid ppl   624.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2142.31 | loss  4.98 | ppl   145.82\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2115.11 | loss  4.91 | ppl   136.19\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2117.29 | loss  4.93 | ppl   138.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 74.58s | valid loss  6.43 | valid ppl   621.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2112.53 | loss  4.96 | ppl   143.04\n",
      " 2000/ 3468 batches | lr 1.00 | ms/batch 2118.26 | loss  4.89 | ppl   133.54\n",
      " 3000/ 3468 batches | lr 1.00 | ms/batch 2122.48 | loss  4.91 | ppl   135.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 74.36s | valid loss  6.44 | valid ppl   625.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |\n",
      "Learning Rate 1\n",
      " 1000/ 3468 batches | lr 1.00 | ms/batch 2103.52 | loss  4.94 | ppl   140.12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b134031df09e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"| epoch {:3d} |\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tgt_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3953\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_tgt_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3953\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UMass/CS 682/Project/learner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, train_tgt, ntokens, lr)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;34mr\"\"\"See :func:`torch.norm`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"nuc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model import RNNModel\n",
    "from learner import Learner\n",
    "\n",
    "device = \"cpu\"\n",
    "model = RNNModel('RNN', 3953, 318, 200, 1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learner = Learner(model, criterion)\n",
    "\n",
    "# Loop over epochs.\n",
    "lr = 1\n",
    "best_val_loss = None\n",
    "\n",
    "# train_data = batchify(torch.Tensor(train_data), 10)\n",
    "# val_data = batchify(torch.Tensor(val_data), 10)\n",
    "# test_data = batchify(torch.Tensor(test_data), 10)\n",
    "train_data_final = torch.Tensor(train_inp).unsqueeze(2)\n",
    "val_data_final = torch.Tensor(val_inp).unsqueeze(2)\n",
    "test_data_final = torch.Tensor(test_inp).unsqueeze(2)\n",
    "\n",
    "train_tgt_final = torch.Tensor(train_tgt).long().unsqueeze(2)\n",
    "val_tgt_final = torch.Tensor(val_tgt).long().unsqueeze(2)\n",
    "test_tgt_final = torch.Tensor(test_tgt).long().unsqueeze(2)\n",
    "\n",
    "print(\"Train\", train_data_final.shape, train_tgt_final.shape)\n",
    "print(\"Val\", val_data_final.shape, val_tgt_final.shape)\n",
    "print(\"Test\", test_data_final.shape, test_tgt_final.shape)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(1, num_epochs):\n",
    "    print (\"| epoch {:3d} |\".format(epoch))\n",
    "    epoch_start_time = time.time()\n",
    "    learner.train(train_data_final, train_tgt_final, 3953, lr)\n",
    "    val_loss = learner.evaluate(val_data_final, val_tgt_final, 3953)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                       val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(\"model.pt\", 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "#     else if lr > 1e-3:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "#         lr /= 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
