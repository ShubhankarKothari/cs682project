{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "## Load libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_glove_file = r\"/Users/anshuman/UMass/CS 682/Project/glove.6B/glove.6b.300d.txt\" ## glove text file that you download from internet, 6b words 300d,\n",
    "new_glove_file =r\"glove6b300d.bin\"\n",
    "movie_data_file =r\"ml-1m/movies.dat\" #movie data file\n",
    "user_data_file =r\"ml-1m/users.dat\" #user data file\n",
    "rating_data_file = r\"ml-1m/ratings.dat\" #rating data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this only once ever, if you have skip this and go to the next cell\n",
    "## Insert your glove location below.\n",
    "## This is done to convert .txt glove to binary format for easier loading.\n",
    "glove_file = datapath(og_glove_file)\n",
    "tmp_file = get_tmpfile(\"glove.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "model.save_word2vec_format(new_glove_file,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshuman/anaconda3/envs/cs682/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    " ##load binary glove model\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format(new_glove_file,binary=True)\n",
    "##get vocabulary\n",
    "vocab = list(glove_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the dataset into dataframes\n",
    "\n",
    "movieData = pd.read_csv(movie_data_file,sep=\"::\",names=[\"MovieID\",\"Name\",\"Genres\"],engine='python')\n",
    "userData = pd.read_csv(user_data_file,sep=\"::\",names=[\"UserID\",\"Gender\",\"Age\",\"Occupation\",\"Zipcode\"],engine='python')\n",
    "ratingData = pd.read_csv(rating_data_file,sep=\"::\",names=[\"UserID\",\"MovieID\",\"Rating\",\"Timestamp\"],engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## movie data preprocessing  - getting a dictionary of movie name and id pairs,\n",
    "##creating genrelist, and reseting movieData dataframe\n",
    "\n",
    "movieNameData = movieData[\"Name\"].unique()\n",
    "movieNameDict = {movieNameData[i]:i for i in range(0,movieNameData.shape[0])}\n",
    "genreList = [\"Action\",\"Adventure\",\"Animation\",\"Children's\",\"Comedy\",\"Crime\",\"Documentary\",\"Drama\",\"Fantasy\",\"Film-Noir\",\n",
    "              \"Horror\",\"Musical\",\"Mystery\",\"Romance\",\"Sci-Fi\",\"Thriller\",\"War\",\"Western\"]\n",
    "genres = movieData[\"Genres\"].apply(lambda x : x.split(\"|\"))\n",
    "movieData.reset_index(inplace=True,drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 3)\n",
      "Movie Embeddings (3953, 318)\n"
     ]
    }
   ],
   "source": [
    "from data_preparation import MovieEncodingEmbedding\n",
    "\n",
    "print(movieData.shape)\n",
    "numRows = movieData.shape[0]\n",
    "# print(movieData, movieData.loc[0][0], movieData.loc[0][1], movieData.loc[0][2])\n",
    "movie_embeddings = np.zeros((3953, 318))\n",
    "\n",
    "# Prepare movie embeddings -> A Mapping from MovieID to its embedding. This can then be used during model training\n",
    "for rowNum in range(numRows):\n",
    "    movieID = movieData.loc[rowNum]['MovieID']\n",
    "#     print(rowNum, movieID)\n",
    "    movie_embeddings[movieID] = MovieEncodingEmbedding(movieID, movieData, genreList, glove_model, vocab)\n",
    "\n",
    "print(\"Movie Embeddings\", movie_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "from data_preparation import UserSequences\n",
    "\n",
    "## get user Sequences\n",
    "userSequences = UserSequences(ratingData,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5421\n",
      "27\n",
      "(3873,)\n"
     ]
    }
   ],
   "source": [
    "print(len(userSequences))\n",
    "print(len(userSequences[0]))\n",
    "print(userSequences[99][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get user-movie encodings for the data with embedding flag false and write it to file.\n",
    "## In every file each line will contain\n",
    "##userid::gender::userDataAge::userDataOccupation::userDataZipCode::movieid::(one hot genre encoding of len 18[each separated by ::]):rating \n",
    "# data = UserMoviePairEncodingBatch(userSequences,False)\n",
    "\n",
    "# with open(\"usermovie.dat\",\"w\") as f:\n",
    "#     for i in range(len(data)):\n",
    "#         for j in range(data[i].shape[0]):\n",
    "#             f.write(\"::\".join(data[i][j]))\n",
    "#             f.write(\"\\n\")\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get user-movie encodings for the data with embedding flag false and write it to file.\n",
    "## In every file each line will contain\n",
    "##userid::gender::userDataAge::userDataOccupation::userDataZipCode::(movieid_embedding[len 300, separated by ::])::(one hot genre encoding of len 18[each separated by ::]):rating \n",
    "\n",
    "# data = UserMoviePairEncodingBatch(userSequences,True)\n",
    "\n",
    "# with open(\"usermoviembeddings.dat\",\"w\") as f:\n",
    "#     for i in range(len(data)):\n",
    "#         for j in range(data[i].shape[0]):\n",
    "#             f.write(\"::\".join(data[i][j]))\n",
    "#             f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3468, 27) (3468, 26, 318) (3468, 26)\n",
      "val (868, 27) (868, 26, 318) (868, 26)\n",
      "test (1085, 27) (1085, 26, 318) (1085, 26)\n"
     ]
    }
   ],
   "source": [
    "# np.array(data).shape\n",
    "# print (\"data\", len(data), data[0].shape)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def fix(data):\n",
    "    newData = np.empty(data.shape, dtype=int)\n",
    "    for i, userSeq in enumerate(data):\n",
    "        for j, tup in enumerate(userSeq):\n",
    "            newData[i][j] = tup[0]\n",
    "    return newData\n",
    "\n",
    "data = fix(np.array(userSequences))\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "train_inp = movie_embeddings[train_data[:, :-1]]\n",
    "train_tgt = train_data[:, 1:]\n",
    "\n",
    "test_inp = movie_embeddings[test_data[:, :-1]]\n",
    "test_tgt = test_data[:, 1:]\n",
    "\n",
    "val_inp = movie_embeddings[val_data[:, :-1]]\n",
    "val_tgt = val_data[:, 1:]\n",
    "\n",
    "print (\"train\", train_data.shape, train_inp.shape, train_tgt.shape)\n",
    "print (\"val\", val_data.shape, val_inp.shape, val_tgt.shape)\n",
    "print (\"test\", test_data.shape, test_inp.shape, test_tgt.shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train torch.Size([3468, 26, 318]) torch.Size([3468, 26])\n",
      "Val torch.Size([868, 26, 318]) torch.Size([868, 26])\n",
      "Test torch.Size([1085, 26, 318]) torch.Size([1085, 26])\n"
     ]
    }
   ],
   "source": [
    "# train_data = batchify(torch.Tensor(train_data), 10)\n",
    "# val_data = batchify(torch.Tensor(val_data), 10)\n",
    "# test_data = batchify(torch.Tensor(test_data), 10)\n",
    "\n",
    "train_data_final = torch.Tensor(train_inp)\n",
    "val_data_final = torch.Tensor(val_inp)\n",
    "test_data_final = torch.Tensor(test_inp)\n",
    "\n",
    "train_tgt_final = torch.Tensor(train_tgt).long()\n",
    "val_tgt_final = torch.Tensor(val_tgt).long()\n",
    "test_tgt_final = torch.Tensor(test_tgt).long()\n",
    "\n",
    "print(\"Train\", train_data_final.shape, train_tgt_final.shape)\n",
    "print(\"Val\", val_data_final.shape, val_tgt_final.shape)\n",
    "print(\"Test\", test_data_final.shape, test_tgt_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |\n",
      "   20/   70 batches  | loss  7.61\n",
      "   40/   70 batches  | loss  7.43\n",
      "   60/   70 batches  | loss  7.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  8.75s | val loss  7.08 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from model import RNNModel\n",
    "from learner import Learner\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "_, temporal_batch_size, input_size = train_data_final.shape\n",
    "ntokens = 3953\n",
    "nlayers = 2\n",
    "hidden_size = 100\n",
    "dropout = 0.5\n",
    "train_batch_size = 50\n",
    "val_batch_size = val_data_final.shape[0]\n",
    "test_batch_size = test_data_final.shape[0]\n",
    "\n",
    "model = RNNModel('gru', ntokens, input_size, hidden_size, nlayers, dropout)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=5)\n",
    "optimizer = optim.SGD(model.parameters(), lr=10)\n",
    "learner = Learner(model, criterion, optimizer)\n",
    "\n",
    "best_val_loss = None\n",
    "num_epochs = 100\n",
    "\n",
    "# Loop over epochs.\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print (\"| epoch {:3d} |\".format(epoch))\n",
    "    epoch_start_time = time.time()\n",
    "    learner.train(train_data_final, train_tgt_final, ntokens, train_batch_size)\n",
    "    val_loss = learner.evaluate(val_data_final, val_tgt_final, ntokens, val_batch_size)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | val loss {:5.2f} '\n",
    "          .format(epoch, (time.time() - epoch_start_time),\n",
    "                                       val_loss))\n",
    "    print('-' * 89)\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(\"model.pt\", 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "#     else if lr > 1e-3:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "#         lr /= 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.plotLearningCurve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_seq = leaner.generate_seq (27, movie_embeddings, 10)\n",
    "print(\"Generated Sequence:\")\n",
    "for id in generated_seq:\n",
    "    print(movieData.loc[movieData[\"MovieID\"]==id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = learner.evaluate(test_data_final, test_tgt_final, ntokens)\n",
    "print(\"Test Loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
