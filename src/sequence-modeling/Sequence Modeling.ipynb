{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "## Load libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_glove_file = r\"/Users/anshuman/UMass/CS 682/Project/glove.6B/glove.6b.300d.txt\" ## glove text file that you download from internet, 6b words 300d,\n",
    "new_glove_file =r\"glove6b300d.bin\"\n",
    "movie_data_file =r\"ml-1m/movies.dat\" #movie data file\n",
    "user_data_file =r\"ml-1m/users.dat\" #user data file\n",
    "rating_data_file = r\"ml-1m/ratings.dat\" #rating data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this only once ever, if you have skip this and go to the next cell\n",
    "## Insert your glove location below.\n",
    "## This is done to convert .txt glove to binary format for easier loading.\n",
    "glove_file = datapath(og_glove_file)\n",
    "tmp_file = get_tmpfile(\"glove.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "model.save_word2vec_format(new_glove_file,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshuman/anaconda3/envs/cs682/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    " ##load binary glove model\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format(new_glove_file,binary=True)\n",
    "##get vocabulary\n",
    "vocab = list(glove_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the dataset into dataframes\n",
    "\n",
    "movieData = pd.read_csv(movie_data_file,sep=\"::\",names=[\"MovieID\",\"Name\",\"Genres\"],engine='python')\n",
    "userData = pd.read_csv(user_data_file,sep=\"::\",names=[\"UserID\",\"Gender\",\"Age\",\"Occupation\",\"Zipcode\"],engine='python')\n",
    "ratingData = pd.read_csv(rating_data_file,sep=\"::\",names=[\"UserID\",\"MovieID\",\"Rating\",\"Timestamp\"],engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## movie data preprocessing  - getting a dictionary of movie name and id pairs,\n",
    "##creating genrelist, and reseting movieData dataframe\n",
    "\n",
    "movieNameData = movieData[\"Name\"].unique()\n",
    "movieNameDict = {movieNameData[i]:i for i in range(0,movieNameData.shape[0])}\n",
    "genreList = [\"Action\",\"Adventure\",\"Animation\",\"Children's\",\"Comedy\",\"Crime\",\"Documentary\",\"Drama\",\"Fantasy\",\"Film-Noir\",\n",
    "              \"Horror\",\"Musical\",\"Mystery\",\"Romance\",\"Sci-Fi\",\"Thriller\",\"War\",\"Western\"]\n",
    "genres = movieData[\"Genres\"].apply(lambda x : x.split(\"|\"))\n",
    "movieData.reset_index(inplace=True,drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 3)\n",
      "Movie Embeddings (3953, 318)\n"
     ]
    }
   ],
   "source": [
    "## Create movie data vector based on embeddings or without.\n",
    "## genre data is on hot encoded since each film could have one or more genre. Array size of the one hot encoding is 18 \n",
    "## structure : movieid:genredata for no  word2vec embedding\n",
    "## structure : movie_name_embedding:genredata for no  word2vec embedding\n",
    "\n",
    "### In MovieEncodingEmbedding(), case-sensitivity is removed and also all characters which are not alpha numeric are removed.\n",
    "### If the word isn't part of glove vocab, a 0 vector is used in its stead.\n",
    "# def MovieEncodingNoEmbedding(movieid):\n",
    "    \n",
    "#     movieDataIDRow = movieData.loc[movieData[\"MovieID\"]==movieid]\n",
    "#     movieDataMovieID = movieNameDict[movieDataIDRow.Name.values[0]]\n",
    "#     movieDataGenreEncoding = [1 if i in movieDataIDRow.Genres.values[0].split(\"|\") else 0  for i in genreList]\n",
    "#     data = [movieDataMovieID]+movieDataGenreEncoding\n",
    "#     return np.array(data)\n",
    "\n",
    "# def MovieEncodingEmbedding(movieid):\n",
    "#     movieDataIDRow = movieData.loc[movieData[\"MovieID\"]==movieid]\n",
    "# #     print(\"Movie name\", movieDataIDRow.Name)\n",
    "#     movieDataMovieID = ''.join([char for char in movieDataIDRow.Name.values[0].lower() if (char.isalpha() or char==\" \")]).split(\" \")\n",
    "#     movieNameEmbedding = np.mean([glove_model[word] if (word in vocab) else np.zeros((1,300)) for word in movieDataMovieID],axis=0)                            \n",
    "#     movieDataGenreEncoding = np.array([1 if i in movieDataIDRow.Genres.values[0].split(\"|\") else 0  for i in genreList])\n",
    "#     data = np.concatenate((movieNameEmbedding, movieDataGenreEncoding.reshape(1,movieDataGenreEncoding.shape[0])), axis=None)\n",
    "#     return data.reshape(1,data.shape[0])\n",
    "\n",
    "from data_preparation import MovieEncodingEmbedding\n",
    "\n",
    "print(movieData.shape)\n",
    "numRows = movieData.shape[0]\n",
    "# print(movieData, movieData.loc[0][0], movieData.loc[0][1], movieData.loc[0][2])\n",
    "movie_embeddings = np.zeros((3953, 318))\n",
    "for rowNum in range(numRows):\n",
    "    movieID = movieData.loc[rowNum]['MovieID']\n",
    "#     print(rowNum, movieID)\n",
    "    movie_embeddings[movieID] = MovieEncodingEmbedding(movieID, movieData, genreList, glove_model, vocab)\n",
    "\n",
    "print(\"Movie Embeddings\", movie_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "from data_preparation import UserSequences\n",
    "\n",
    "## get user Sequences\n",
    "userSequences = UserSequences(ratingData,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5421\n",
      "27\n",
      "(3873,)\n"
     ]
    }
   ],
   "source": [
    "print(len(userSequences))\n",
    "print(len(userSequences[0]))\n",
    "print(userSequences[99][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get user-movie encodings for the data with embedding flag false and write it to file.\n",
    "## In every file each line will contain\n",
    "##userid::gender::userDataAge::userDataOccupation::userDataZipCode::movieid::(one hot genre encoding of len 18[each separated by ::]):rating \n",
    "# data = UserMoviePairEncodingBatch(userSequences,False)\n",
    "\n",
    "# with open(\"usermovie.dat\",\"w\") as f:\n",
    "#     for i in range(len(data)):\n",
    "#         for j in range(data[i].shape[0]):\n",
    "#             f.write(\"::\".join(data[i][j]))\n",
    "#             f.write(\"\\n\")\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get user-movie encodings for the data with embedding flag false and write it to file.\n",
    "## In every file each line will contain\n",
    "##userid::gender::userDataAge::userDataOccupation::userDataZipCode::(movieid_embedding[len 300, separated by ::])::(one hot genre encoding of len 18[each separated by ::]):rating \n",
    "\n",
    "# data = UserMoviePairEncodingBatch(userSequences,True)\n",
    "\n",
    "# with open(\"usermoviembeddings.dat\",\"w\") as f:\n",
    "#     for i in range(len(data)):\n",
    "#         for j in range(data[i].shape[0]):\n",
    "#             f.write(\"::\".join(data[i][j]))\n",
    "#             f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3468, 27) (3468, 26, 318) (3468, 26)\n",
      "val (868, 27) (868, 26, 318) (868, 26)\n",
      "test (1085, 27) (1085, 26, 318) (1085, 26)\n"
     ]
    }
   ],
   "source": [
    "# np.array(data).shape\n",
    "# print (\"data\", len(data), data[0].shape)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def fix(data):\n",
    "    newData = np.empty(data.shape, dtype=int)\n",
    "    for i, userSeq in enumerate(data):\n",
    "        for j, tup in enumerate(userSeq):\n",
    "            newData[i][j] = tup[0]\n",
    "    return newData\n",
    "\n",
    "data = fix(np.array(userSequences))\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "train_inp = movie_embeddings[train_data[:, :-1]]\n",
    "train_tgt = train_data[:, 1:]\n",
    "\n",
    "test_inp = movie_embeddings[test_data[:, :-1]]\n",
    "test_tgt = test_data[:, 1:]\n",
    "\n",
    "val_inp = movie_embeddings[val_data[:, :-1]]\n",
    "val_tgt = val_data[:, 1:]\n",
    "\n",
    "print (\"train\", train_data.shape, train_inp.shape, train_tgt.shape)\n",
    "print (\"val\", val_data.shape, val_inp.shape, val_tgt.shape)\n",
    "print (\"test\", test_data.shape, test_inp.shape, test_tgt.shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train torch.Size([3468, 26, 318]) torch.Size([3468, 26])\n",
      "Val torch.Size([868, 26, 318]) torch.Size([868, 26])\n",
      "Test torch.Size([1085, 26, 318]) torch.Size([1085, 26])\n"
     ]
    }
   ],
   "source": [
    "# train_data = batchify(torch.Tensor(train_data), 10)\n",
    "# val_data = batchify(torch.Tensor(val_data), 10)\n",
    "# test_data = batchify(torch.Tensor(test_data), 10)\n",
    "\n",
    "train_data_final = torch.Tensor(train_inp)\n",
    "val_data_final = torch.Tensor(val_inp)\n",
    "test_data_final = torch.Tensor(test_inp)\n",
    "\n",
    "train_tgt_final = torch.Tensor(train_tgt).long()\n",
    "val_tgt_final = torch.Tensor(val_tgt).long()\n",
    "test_tgt_final = torch.Tensor(test_tgt).long()\n",
    "\n",
    "print(\"Train\", train_data_final.shape, train_tgt_final.shape)\n",
    "print(\"Val\", val_data_final.shape, val_tgt_final.shape)\n",
    "print(\"Test\", test_data_final.shape, test_tgt_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |\n",
      "   20/   70 batches  | loss  7.60\n",
      "   40/   70 batches  | loss  7.40\n",
      "   60/   70 batches  | loss  7.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  4.37s | val loss  7.01 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |\n",
      "   20/   70 batches  | loss  7.07\n",
      "   40/   70 batches  | loss  7.05\n",
      "   60/   70 batches  | loss  7.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  4.36s | val loss  6.93 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |\n",
      "   20/   70 batches  | loss  6.96\n",
      "   40/   70 batches  | loss  6.95\n",
      "   60/   70 batches  | loss  6.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  4.34s | val loss  6.86 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |\n",
      "   20/   70 batches  | loss  6.89\n",
      "   40/   70 batches  | loss  6.88\n",
      "   60/   70 batches  | loss  6.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  4.35s | val loss  6.81 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |\n",
      "   20/   70 batches  | loss  6.83\n",
      "   40/   70 batches  | loss  6.82\n",
      "   60/   70 batches  | loss  6.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  4.37s | val loss  6.72 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |\n",
      "   20/   70 batches  | loss  6.75\n",
      "   40/   70 batches  | loss  6.75\n",
      "   60/   70 batches  | loss  6.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  4.51s | val loss  6.65 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |\n",
      "   20/   70 batches  | loss  6.69\n",
      "   40/   70 batches  | loss  6.69\n",
      "   60/   70 batches  | loss  6.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  4.41s | val loss  6.59 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |\n",
      "   20/   70 batches  | loss  6.65\n",
      "   40/   70 batches  | loss  6.65\n",
      "   60/   70 batches  | loss  6.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  4.38s | val loss  6.56 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |\n",
      "   20/   70 batches  | loss  6.61\n",
      "   40/   70 batches  | loss  6.61\n",
      "   60/   70 batches  | loss  6.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  4.41s | val loss  6.53 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |\n",
      "   20/   70 batches  | loss  6.58\n",
      "   40/   70 batches  | loss  6.58\n",
      "   60/   70 batches  | loss  6.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  4.41s | val loss  6.52 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |\n",
      "   20/   70 batches  | loss  6.56\n",
      "   40/   70 batches  | loss  6.56\n",
      "   60/   70 batches  | loss  6.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  4.49s | val loss  6.49 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |\n",
      "   20/   70 batches  | loss  6.53\n",
      "   40/   70 batches  | loss  6.54\n",
      "   60/   70 batches  | loss  6.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  4.38s | val loss  6.47 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |\n",
      "   20/   70 batches  | loss  6.51\n",
      "   40/   70 batches  | loss  6.52\n",
      "   60/   70 batches  | loss  6.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  4.40s | val loss  6.45 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |\n",
      "   20/   70 batches  | loss  6.49\n",
      "   40/   70 batches  | loss  6.50\n",
      "   60/   70 batches  | loss  6.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  4.48s | val loss  6.45 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |\n",
      "   20/   70 batches  | loss  6.47\n",
      "   40/   70 batches  | loss  6.48\n",
      "   60/   70 batches  | loss  6.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  4.39s | val loss  6.44 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |\n",
      "   20/   70 batches  | loss  6.46\n",
      "   40/   70 batches  | loss  6.47\n",
      "   60/   70 batches  | loss  6.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  4.44s | val loss  6.42 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |\n",
      "   20/   70 batches  | loss  6.44\n",
      "   40/   70 batches  | loss  6.45\n",
      "   60/   70 batches  | loss  6.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  4.43s | val loss  6.42 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |\n",
      "   20/   70 batches  | loss  6.43\n",
      "   40/   70 batches  | loss  6.44\n",
      "   60/   70 batches  | loss  6.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  4.37s | val loss  6.42 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |\n",
      "   20/   70 batches  | loss  6.41\n",
      "   40/   70 batches  | loss  6.43\n",
      "   60/   70 batches  | loss  6.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  4.45s | val loss  6.40 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |\n",
      "   20/   70 batches  | loss  6.40\n",
      "   40/   70 batches  | loss  6.41\n",
      "   60/   70 batches  | loss  6.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  4.38s | val loss  6.39 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |\n",
      "   20/   70 batches  | loss  6.39\n",
      "   40/   70 batches  | loss  6.41\n",
      "   60/   70 batches  | loss  6.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  4.37s | val loss  6.39 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |\n",
      "   20/   70 batches  | loss  6.39\n",
      "   40/   70 batches  | loss  6.40\n",
      "   60/   70 batches  | loss  6.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  4.38s | val loss  6.38 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |\n",
      "   20/   70 batches  | loss  6.36\n",
      "   40/   70 batches  | loss  6.38\n",
      "   60/   70 batches  | loss  6.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  4.37s | val loss  6.37 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |\n",
      "   20/   70 batches  | loss  6.36\n",
      "   40/   70 batches  | loss  6.38\n",
      "   60/   70 batches  | loss  6.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  4.35s | val loss  6.37 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20/   70 batches  | loss  6.35\n",
      "   40/   70 batches  | loss  6.37\n",
      "   60/   70 batches  | loss  6.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  4.33s | val loss  6.37 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |\n",
      "   20/   70 batches  | loss  6.34\n",
      "   40/   70 batches  | loss  6.36\n",
      "   60/   70 batches  | loss  6.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  4.48s | val loss  6.36 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |\n",
      "   20/   70 batches  | loss  6.33\n",
      "   40/   70 batches  | loss  6.35\n",
      "   60/   70 batches  | loss  6.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  4.43s | val loss  6.39 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |\n",
      "   20/   70 batches  | loss  6.32\n",
      "   40/   70 batches  | loss  6.34\n",
      "   60/   70 batches  | loss  6.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  4.37s | val loss  6.37 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |\n",
      "   20/   70 batches  | loss  6.31\n",
      "   40/   70 batches  | loss  6.34\n",
      "   60/   70 batches  | loss  6.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  4.39s | val loss  6.36 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |\n",
      "   20/   70 batches  | loss  6.30\n",
      "   40/   70 batches  | loss  6.33\n",
      "   60/   70 batches  | loss  6.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  4.39s | val loss  6.35 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |\n",
      "   20/   70 batches  | loss  6.31\n",
      "   40/   70 batches  | loss  6.33\n",
      "   60/   70 batches  | loss  6.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  4.44s | val loss  6.34 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |\n",
      "   20/   70 batches  | loss  6.29\n",
      "   40/   70 batches  | loss  6.32\n",
      "   60/   70 batches  | loss  6.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  4.50s | val loss  6.36 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |\n",
      "   20/   70 batches  | loss  6.29\n",
      "   40/   70 batches  | loss  6.31\n",
      "   60/   70 batches  | loss  6.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  4.41s | val loss  6.34 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |\n",
      "   20/   70 batches  | loss  6.28\n",
      "   40/   70 batches  | loss  6.30\n",
      "   60/   70 batches  | loss  6.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  4.43s | val loss  6.34 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |\n",
      "   20/   70 batches  | loss  6.27\n",
      "   40/   70 batches  | loss  6.30\n",
      "   60/   70 batches  | loss  6.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time:  4.41s | val loss  6.35 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |\n",
      "   20/   70 batches  | loss  6.28\n",
      "   40/   70 batches  | loss  6.30\n",
      "   60/   70 batches  | loss  6.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  4.54s | val loss  6.38 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from model import RNNModel\n",
    "from learner import Learner\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "_, temporal_batch_size, input_size = train_data_final.shape\n",
    "ntokens = 3953\n",
    "nlayers = 1\n",
    "hidden_size = 25\n",
    "dropout = 0.5\n",
    "\n",
    "model = RNNModel('gru', ntokens, input_size, hidden_size, nlayers, dropout, temporal_batch_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=5)\n",
    "optimizer = optim.SGD(model.parameters(), lr=10)\n",
    "learner = Learner(model, criterion, optimizer)\n",
    "\n",
    "best_val_loss = None\n",
    "num_epochs = 50\n",
    "\n",
    "# Loop over epochs.\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print (\"| epoch {:3d} |\".format(epoch))\n",
    "    epoch_start_time = time.time()\n",
    "    learner.train(train_data_final, train_tgt_final, ntokens, batch_size=50)\n",
    "    val_loss = learner.evaluate(val_data_final, val_tgt_final, ntokens)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | val loss {:5.2f} '\n",
    "          .format(epoch, (time.time() - epoch_start_time),\n",
    "                                       val_loss))\n",
    "    print('-' * 89)\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(\"model.pt\", 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "#     else if lr > 1e-3:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "#         lr /= 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.plotLearningCurve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
