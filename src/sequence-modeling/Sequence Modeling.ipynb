{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "## Load libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_glove_file = r\"/Users/anshuman/UMass/CS 682/Project/glove.6B/glove.6b.300d.txt\" ## glove text file that you download from internet, 6b words 300d,\n",
    "new_glove_file =r\"glove6b300d.bin\"\n",
    "movie_data_file =r\"ml-1m/movies.dat\" #movie data file\n",
    "user_data_file =r\"ml-1m/users.dat\" #user data file\n",
    "rating_data_file = r\"ml-1m/ratings.dat\" #rating data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this only once ever, if you have skip this and go to the next cell\n",
    "## Insert your glove location below.\n",
    "## This is done to convert .txt glove to binary format for easier loading.\n",
    "glove_file = datapath(og_glove_file)\n",
    "tmp_file = get_tmpfile(\"glove.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "model.save_word2vec_format(new_glove_file,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshuman/anaconda3/envs/cs682/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    " ##load binary glove model\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format(new_glove_file,binary=True)\n",
    "##get vocabulary\n",
    "vocab = list(glove_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the dataset into dataframes\n",
    "\n",
    "movieData = pd.read_csv(movie_data_file,sep=\"::\",names=[\"MovieID\",\"Name\",\"Genres\"],engine='python')\n",
    "userData = pd.read_csv(user_data_file,sep=\"::\",names=[\"UserID\",\"Gender\",\"Age\",\"Occupation\",\"Zipcode\"],engine='python')\n",
    "ratingData = pd.read_csv(rating_data_file,sep=\"::\",names=[\"UserID\",\"MovieID\",\"Rating\",\"Timestamp\"],engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## movie data preprocessing  - getting a dictionary of movie name and id pairs,\n",
    "##creating genrelist, and reseting movieData dataframe\n",
    "\n",
    "movieNameData = movieData[\"Name\"].unique()\n",
    "movieNameDict = {movieNameData[i]:i for i in range(0,movieNameData.shape[0])}\n",
    "genreList = [\"Action\",\"Adventure\",\"Animation\",\"Children's\",\"Comedy\",\"Crime\",\"Documentary\",\"Drama\",\"Fantasy\",\"Film-Noir\",\n",
    "              \"Horror\",\"Musical\",\"Mystery\",\"Romance\",\"Sci-Fi\",\"Thriller\",\"War\",\"Western\"]\n",
    "genres = movieData[\"Genres\"].apply(lambda x : x.split(\"|\"))\n",
    "movieData.reset_index(inplace=True,drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 3)\n",
      "Movie Embeddings (3953, 318)\n"
     ]
    }
   ],
   "source": [
    "## Create movie data vector based on embeddings or without.\n",
    "## genre data is on hot encoded since each film could have one or more genre. Array size of the one hot encoding is 18 \n",
    "## structure : movieid:genredata for no  word2vec embedding\n",
    "## structure : movie_name_embedding:genredata for no  word2vec embedding\n",
    "\n",
    "### In MovieEncodingEmbedding(), case-sensitivity is removed and also all characters which are not alpha numeric are removed.\n",
    "### If the word isn't part of glove vocab, a 0 vector is used in its stead.\n",
    "# def MovieEncodingNoEmbedding(movieid):\n",
    "    \n",
    "#     movieDataIDRow = movieData.loc[movieData[\"MovieID\"]==movieid]\n",
    "#     movieDataMovieID = movieNameDict[movieDataIDRow.Name.values[0]]\n",
    "#     movieDataGenreEncoding = [1 if i in movieDataIDRow.Genres.values[0].split(\"|\") else 0  for i in genreList]\n",
    "#     data = [movieDataMovieID]+movieDataGenreEncoding\n",
    "#     return np.array(data)\n",
    "\n",
    "# def MovieEncodingEmbedding(movieid):\n",
    "#     movieDataIDRow = movieData.loc[movieData[\"MovieID\"]==movieid]\n",
    "# #     print(\"Movie name\", movieDataIDRow.Name)\n",
    "#     movieDataMovieID = ''.join([char for char in movieDataIDRow.Name.values[0].lower() if (char.isalpha() or char==\" \")]).split(\" \")\n",
    "#     movieNameEmbedding = np.mean([glove_model[word] if (word in vocab) else np.zeros((1,300)) for word in movieDataMovieID],axis=0)                            \n",
    "#     movieDataGenreEncoding = np.array([1 if i in movieDataIDRow.Genres.values[0].split(\"|\") else 0  for i in genreList])\n",
    "#     data = np.concatenate((movieNameEmbedding, movieDataGenreEncoding.reshape(1,movieDataGenreEncoding.shape[0])), axis=None)\n",
    "#     return data.reshape(1,data.shape[0])\n",
    "\n",
    "from data_preparation import MovieEncodingEmbedding\n",
    "\n",
    "print(movieData.shape)\n",
    "numRows = movieData.shape[0]\n",
    "# print(movieData, movieData.loc[0][0], movieData.loc[0][1], movieData.loc[0][2])\n",
    "movie_embeddings = np.zeros((3953, 318))\n",
    "for rowNum in range(numRows):\n",
    "    movieID = movieData.loc[rowNum]['MovieID']\n",
    "#     print(rowNum, movieID)\n",
    "    movie_embeddings[movieID] = MovieEncodingEmbedding(movieID, movieData, genreList, glove_model, vocab)\n",
    "\n",
    "print(\"Movie Embeddings\", movie_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "from data_preparation import UserSequences\n",
    "\n",
    "## get user Sequences\n",
    "userSequences = UserSequences(ratingData,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5421\n",
      "27\n",
      "(3873,)\n"
     ]
    }
   ],
   "source": [
    "print(len(userSequences))\n",
    "print(len(userSequences[0]))\n",
    "print(userSequences[99][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get user-movie encodings for the data with embedding flag false and write it to file.\n",
    "## In every file each line will contain\n",
    "##userid::gender::userDataAge::userDataOccupation::userDataZipCode::movieid::(one hot genre encoding of len 18[each separated by ::]):rating \n",
    "# data = UserMoviePairEncodingBatch(userSequences,False)\n",
    "\n",
    "# with open(\"usermovie.dat\",\"w\") as f:\n",
    "#     for i in range(len(data)):\n",
    "#         for j in range(data[i].shape[0]):\n",
    "#             f.write(\"::\".join(data[i][j]))\n",
    "#             f.write(\"\\n\")\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get user-movie encodings for the data with embedding flag false and write it to file.\n",
    "## In every file each line will contain\n",
    "##userid::gender::userDataAge::userDataOccupation::userDataZipCode::(movieid_embedding[len 300, separated by ::])::(one hot genre encoding of len 18[each separated by ::]):rating \n",
    "\n",
    "# data = UserMoviePairEncodingBatch(userSequences,True)\n",
    "\n",
    "# with open(\"usermoviembeddings.dat\",\"w\") as f:\n",
    "#     for i in range(len(data)):\n",
    "#         for j in range(data[i].shape[0]):\n",
    "#             f.write(\"::\".join(data[i][j]))\n",
    "#             f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3468, 27) (3468, 26, 318) (3468, 26)\n",
      "val (868, 27) (868, 26, 318) (868, 26)\n",
      "test (1085, 27) (1085, 26, 318) (1085, 26)\n"
     ]
    }
   ],
   "source": [
    "# np.array(data).shape\n",
    "# print (\"data\", len(data), data[0].shape)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def fix(data):\n",
    "    newData = np.empty(data.shape, dtype=int)\n",
    "    for i, userSeq in enumerate(data):\n",
    "        for j, tup in enumerate(userSeq):\n",
    "            newData[i][j] = tup[0]\n",
    "    return newData\n",
    "\n",
    "data = fix(np.array(userSequences))\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "train_inp = movie_embeddings[train_data[:, :-1]]\n",
    "train_tgt = train_data[:, 1:]\n",
    "\n",
    "test_inp = movie_embeddings[test_data[:, :-1]]\n",
    "test_tgt = test_data[:, 1:]\n",
    "\n",
    "val_inp = movie_embeddings[val_data[:, :-1]]\n",
    "val_tgt = val_data[:, 1:]\n",
    "\n",
    "print (\"train\", train_data.shape, train_inp.shape, train_tgt.shape)\n",
    "print (\"val\", val_data.shape, val_inp.shape, val_tgt.shape)\n",
    "print (\"test\", test_data.shape, test_inp.shape, test_tgt.shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train torch.Size([3468, 26, 318]) torch.Size([3468, 26])\n",
      "Val torch.Size([868, 26, 318]) torch.Size([868, 26])\n",
      "Test torch.Size([1085, 26, 318]) torch.Size([1085, 26])\n"
     ]
    }
   ],
   "source": [
    "# train_data = batchify(torch.Tensor(train_data), 10)\n",
    "# val_data = batchify(torch.Tensor(val_data), 10)\n",
    "# test_data = batchify(torch.Tensor(test_data), 10)\n",
    "\n",
    "train_data_final = torch.Tensor(train_inp)\n",
    "val_data_final = torch.Tensor(val_inp)\n",
    "test_data_final = torch.Tensor(test_inp)\n",
    "\n",
    "train_tgt_final = torch.Tensor(train_tgt).long()\n",
    "val_tgt_final = torch.Tensor(val_tgt).long()\n",
    "test_tgt_final = torch.Tensor(test_tgt).long()\n",
    "\n",
    "print(\"Train\", train_data_final.shape, train_tgt_final.shape)\n",
    "print(\"Val\", val_data_final.shape, val_tgt_final.shape)\n",
    "print(\"Test\", test_data_final.shape, test_tgt_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |\n",
      "  100/  347 batches | lr 1.00 | loss  7.43\n",
      "  200/  347 batches | lr 1.00 | loss  7.26\n",
      "  300/  347 batches | lr 1.00 | loss  7.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 12.94s | val loss  7.04 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |\n",
      "  100/  347 batches | lr 1.00 | loss  7.02\n",
      "  200/  347 batches | lr 1.00 | loss  7.01\n",
      "  300/  347 batches | lr 1.00 | loss  7.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 13.23s | val loss  7.02 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.99\n",
      "  200/  347 batches | lr 1.00 | loss  6.98\n",
      "  300/  347 batches | lr 1.00 | loss  6.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 13.40s | val loss  6.98 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.92\n",
      "  200/  347 batches | lr 1.00 | loss  6.91\n",
      "  300/  347 batches | lr 1.00 | loss  6.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 13.34s | val loss  6.92 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.80\n",
      "  200/  347 batches | lr 1.00 | loss  6.79\n",
      "  300/  347 batches | lr 1.00 | loss  6.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 13.35s | val loss  7.10 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.72\n",
      "  200/  347 batches | lr 1.00 | loss  6.71\n",
      "  300/  347 batches | lr 1.00 | loss  6.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 13.31s | val loss  6.75 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.64\n",
      "  200/  347 batches | lr 1.00 | loss  6.63\n",
      "  300/  347 batches | lr 1.00 | loss  6.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 13.34s | val loss  7.22 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.58\n",
      "  200/  347 batches | lr 1.00 | loss  6.57\n",
      "  300/  347 batches | lr 1.00 | loss  6.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 13.16s | val loss  6.78 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.50\n",
      "  200/  347 batches | lr 1.00 | loss  6.49\n",
      "  300/  347 batches | lr 1.00 | loss  6.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 12.98s | val loss  6.76 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.43\n",
      "  200/  347 batches | lr 1.00 | loss  6.43\n",
      "  300/  347 batches | lr 1.00 | loss  6.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 12.92s | val loss  6.74 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.38\n",
      "  200/  347 batches | lr 1.00 | loss  6.37\n",
      "  300/  347 batches | lr 1.00 | loss  6.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 13.06s | val loss  6.72 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.32\n",
      "  200/  347 batches | lr 1.00 | loss  6.32\n",
      "  300/  347 batches | lr 1.00 | loss  6.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 13.08s | val loss  6.71 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.28\n",
      "  200/  347 batches | lr 1.00 | loss  6.27\n",
      "  300/  347 batches | lr 1.00 | loss  6.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 12.87s | val loss  6.70 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.22\n",
      "  200/  347 batches | lr 1.00 | loss  6.21\n",
      "  300/  347 batches | lr 1.00 | loss  6.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 13.01s | val loss  6.70 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.17\n",
      "  200/  347 batches | lr 1.00 | loss  6.16\n",
      "  300/  347 batches | lr 1.00 | loss  6.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 13.13s | val loss  6.69 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.15\n",
      "  200/  347 batches | lr 1.00 | loss  6.13\n",
      "  300/  347 batches | lr 1.00 | loss  6.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 13.04s | val loss  6.56 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.07\n",
      "  200/  347 batches | lr 1.00 | loss  6.07\n",
      "  300/  347 batches | lr 1.00 | loss  6.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 13.14s | val loss  6.69 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.01\n",
      "  200/  347 batches | lr 1.00 | loss  6.02\n",
      "  300/  347 batches | lr 1.00 | loss  6.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 13.04s | val loss  6.69 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.97\n",
      "  200/  347 batches | lr 1.00 | loss  5.98\n",
      "  300/  347 batches | lr 1.00 | loss  5.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 12.94s | val loss  6.70 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.92\n",
      "  200/  347 batches | lr 1.00 | loss  5.92\n",
      "  300/  347 batches | lr 1.00 | loss  5.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 12.88s | val loss  6.67 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.87\n",
      "  200/  347 batches | lr 1.00 | loss  5.88\n",
      "  300/  347 batches | lr 1.00 | loss  5.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 12.97s | val loss  6.67 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.82\n",
      "  200/  347 batches | lr 1.00 | loss  5.83\n",
      "  300/  347 batches | lr 1.00 | loss  5.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 13.24s | val loss  6.73 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/  347 batches | lr 1.00 | loss  5.78\n",
      "  200/  347 batches | lr 1.00 | loss  5.79\n",
      "  300/  347 batches | lr 1.00 | loss  5.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 12.97s | val loss  6.69 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.73\n",
      "  200/  347 batches | lr 1.00 | loss  5.74\n",
      "  300/  347 batches | lr 1.00 | loss  5.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 13.00s | val loss  6.76 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.69\n",
      "  200/  347 batches | lr 1.00 | loss  5.70\n",
      "  300/  347 batches | lr 1.00 | loss  5.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 12.97s | val loss  6.77 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.64\n",
      "  200/  347 batches | lr 1.00 | loss  5.65\n",
      "  300/  347 batches | lr 1.00 | loss  5.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 12.96s | val loss  6.74 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.59\n",
      "  200/  347 batches | lr 1.00 | loss  5.61\n",
      "  300/  347 batches | lr 1.00 | loss  5.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 12.88s | val loss  6.80 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.55\n",
      "  200/  347 batches | lr 1.00 | loss  5.57\n",
      "  300/  347 batches | lr 1.00 | loss  5.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 12.83s | val loss  6.81 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.51\n",
      "  200/  347 batches | lr 1.00 | loss  5.52\n",
      "  300/  347 batches | lr 1.00 | loss  5.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 13.11s | val loss  6.78 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.47\n",
      "  200/  347 batches | lr 1.00 | loss  5.48\n",
      "  300/  347 batches | lr 1.00 | loss  5.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 12.93s | val loss  6.79 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.43\n",
      "  200/  347 batches | lr 1.00 | loss  5.44\n",
      "  300/  347 batches | lr 1.00 | loss  5.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 12.82s | val loss  6.81 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.39\n",
      "  200/  347 batches | lr 1.00 | loss  5.40\n",
      "  300/  347 batches | lr 1.00 | loss  5.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 12.96s | val loss  6.80 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.35\n",
      "  200/  347 batches | lr 1.00 | loss  5.36\n",
      "  300/  347 batches | lr 1.00 | loss  5.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 12.88s | val loss  6.82 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.30\n",
      "  200/  347 batches | lr 1.00 | loss  5.32\n",
      "  300/  347 batches | lr 1.00 | loss  5.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 12.98s | val loss  6.84 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.27\n",
      "  200/  347 batches | lr 1.00 | loss  5.28\n",
      "  300/  347 batches | lr 1.00 | loss  5.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 12.97s | val loss  6.85 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.23\n",
      "  200/  347 batches | lr 1.00 | loss  5.25\n",
      "  300/  347 batches | lr 1.00 | loss  5.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 13.06s | val loss  6.87 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.19\n",
      "  200/  347 batches | lr 1.00 | loss  5.21\n",
      "  300/  347 batches | lr 1.00 | loss  5.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 12.92s | val loss  6.88 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.15\n",
      "  200/  347 batches | lr 1.00 | loss  5.18\n",
      "  300/  347 batches | lr 1.00 | loss  5.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 12.91s | val loss  6.90 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.12\n",
      "  200/  347 batches | lr 1.00 | loss  5.14\n",
      "  300/  347 batches | lr 1.00 | loss  5.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 13.05s | val loss  6.92 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.08\n",
      "  200/  347 batches | lr 1.00 | loss  5.11\n",
      "  300/  347 batches | lr 1.00 | loss  5.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 12.95s | val loss  6.93 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.05\n",
      "  200/  347 batches | lr 1.00 | loss  5.08\n",
      "  300/  347 batches | lr 1.00 | loss  5.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 12.98s | val loss  6.94 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.02\n",
      "  200/  347 batches | lr 1.00 | loss  5.04\n",
      "  300/  347 batches | lr 1.00 | loss  5.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 12.93s | val loss  6.94 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |\n",
      "  100/  347 batches | lr 1.00 | loss  4.98\n",
      "  200/  347 batches | lr 1.00 | loss  5.01\n",
      "  300/  347 batches | lr 1.00 | loss  5.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 12.92s | val loss  6.92 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |\n",
      "  100/  347 batches | lr 1.00 | loss  4.95\n",
      "  200/  347 batches | lr 1.00 | loss  4.98\n",
      "  300/  347 batches | lr 1.00 | loss  4.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 12.87s | val loss  6.94 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/  347 batches | lr 1.00 | loss  4.92\n",
      "  200/  347 batches | lr 1.00 | loss  4.95\n",
      "  300/  347 batches | lr 1.00 | loss  4.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 12.98s | val loss  6.94 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |\n",
      "  100/  347 batches | lr 1.00 | loss  4.89\n",
      "  200/  347 batches | lr 1.00 | loss  4.92\n",
      "  300/  347 batches | lr 1.00 | loss  4.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 13.15s | val loss  6.96 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |\n",
      "  100/  347 batches | lr 1.00 | loss  4.86\n",
      "  200/  347 batches | lr 1.00 | loss  4.90\n",
      "  300/  347 batches | lr 1.00 | loss  4.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 13.36s | val loss  6.97 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |\n",
      "  100/  347 batches | lr 1.00 | loss  4.84\n",
      "  200/  347 batches | lr 1.00 | loss  4.88\n",
      "  300/  347 batches | lr 1.00 | loss  4.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 13.15s | val loss  6.99 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |\n",
      "  100/  347 batches | lr 1.00 | loss  4.81\n",
      "  200/  347 batches | lr 1.00 | loss  4.85\n",
      "  300/  347 batches | lr 1.00 | loss  4.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 13.14s | val loss  6.99 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |\n",
      "  100/  347 batches | lr 1.00 | loss  4.79\n",
      "  200/  347 batches | lr 1.00 | loss  4.82\n",
      "  300/  347 batches | lr 1.00 | loss  4.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 13.20s | val loss  7.01 \n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from model import RNNModel\n",
    "from learner import Learner\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "_, temporal_batch_size, input_size = train_data_final.shape\n",
    "ntokens = 3953\n",
    "nlayers = 2\n",
    "hidden_size = 200\n",
    "\n",
    "model = RNNModel('rnn', ntokens, input_size, hidden_size, nlayers, temporal_batch_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learner = Learner(model, criterion)\n",
    "\n",
    "best_val_loss = None\n",
    "num_epochs = 50\n",
    "\n",
    "# Loop over epochs.\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print (\"| epoch {:3d} |\".format(epoch))\n",
    "    epoch_start_time = time.time()\n",
    "    learner.train(train_data_final, train_tgt_final, ntokens, lr=1, batch_size=10)\n",
    "    val_loss = learner.evaluate(val_data_final, val_tgt_final, ntokens)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | val loss {:5.2f} '\n",
    "          .format(epoch, (time.time() - epoch_start_time),\n",
    "                                       val_loss))\n",
    "    print('-' * 89)\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(\"model.pt\", 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "#     else if lr > 1e-3:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "#         lr /= 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAACCCAYAAABvuIK4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3jc1Z3v8feZ0aiPepesalmybLnb2BhijCmm2SGXAAayLEmWsNm9N5uEm+vlSVjIZvdhc7MJ7KbcEHoSbAiJIQmhGHCwccHYcpctq1u99y7NuX+cUbGRsWVrNCPN9/U880z/zTn26KOj7++c309prRFCCOG5LO5ugBBCiM8mQS2EEB5OgloIITycBLUQQng4CWohhPBwEtRCCOHhfFyx0aioKJ2amuqKTQshxIx08ODBRq119HjPuSSoU1NTOXDggCs2LYQQM5JSqvx8z0npQwghPJwE9YV0NUJblbtbIYTwYhLUF/LaA/D8TeBwuLslQggv5VFB/acj1dS197q7GaPaKqF0J7SWQ9kud7dGCOGlPCaoW7r6eWTbMb74//ZS0dzt7uYYx14z17ZAOPyye9sihPBaHhPU4UG+/PorV9DWM8Cdv9xLcUOnu5sEx34HScthwZ2Q/wb0tru7RUIIL+QxQQ2waFYYWx9cycCQg7t+uZeTNZMcjE3F8PzNcGbfhV9blw91xyH3Tlh0Hwz2QP7rk9seIYS4CBcMaqVUllLq8JhLu1Lqn1zVoLnxIbzytVXYrBbufnofhytaJ2fDrWfgxQ1Qvhu2PwoXOg73sd+BssK82yFpGURmSvlDCOEWFwxqrXWB1nqR1noRsBToBra5pDVb7oHf/x0ZR3/Mm1cWcr3vMR791WvsL6i4vO121JqQ7u+AZV+Bio+hfM/5X+9wmPp0xloIjgalYNE9cGavGZULIcQUmujKxHVAsdb6vCtoLpnDAb1tUHsMjv+eCD3Ej8D8KtnybToDErFkriMw+3pI+xwEhF3cdrua4KWN0FkPf/MGxM2Hk3+Ej34MqavHf0/lfmg7A9d+d/SxhXfDB/9qRtXrvneZnRVCTGtam7zqaoSuBuh2XjuGYMXfTfrHTTSo7wa2THorACwWeOBNc9sxBB010FpBR30p2z7YS3znCVYdeRWOvsQQVlrDc/GZcx2huTdBwhLz/nP1tsFvboeWMrj3NZi13Dy+8u/h/e9DzRGIX/jp9x19FXwCIPvm0cdCEiDjWjiyBdY+AhbrpP8TCCGmWH8XdNY5A7cRupucodsI3c3Q125eM9Btrvs7ob8belrAMfDp7QWEuySo1cWeM1Ep5QtUA/O01nXjPP8g8CBAcnLy0vLyyRt0Dzk0J6rb2F9UR1PBbsJrdrHCcZgFqhSL0vT4ReGXcwuW7JshfQ3YAsw/6q9vh6o8uPtlmHPD6AZ72+An82H2OvjiC+d82AD8aI4pe9zx3NnPHf+DWQDzpW0mtIUQF29oAIb6weoLFh9TUnSF4dFuRy101kJH3fmv+zvG34ZPAARFgZ8dfIPMFF3fYPANNLcDws3zQdFjrqMhMAp8fC+p2Uqpg1rrZeM+N4Gg3gj8g9b6hgu9dtmyZdqVB2VyODSnajs4XFBM1YE/Mbd9N2utRwiiB20LRGVca34zVnwMdzwP8z7/6Y289xjsfgr+8QBEZow+fvodePlO2PQKZK0/+z0DvfCfcyDzBvgfz7isf0J4vKFB6Ko3f/l21Dmvh4Ox1oxG+7vGjEK7YKhvzAYU+PiB1c8Em08ABMeAPQ7s8RASb66DYsyotrPOuf260c8c6AbtMBfH0Ojt/k4YHGfhnC0I7LEQHHfOdawzZCNN6AZGmUCeYp8V1BMpfWzCVWWPCbJYFDkJIeQkLEZfs4j3T9bzpffzCa7Zy+3WI9xYup+AvgbU538+fkgDrPw67PsF7H4SNvz36ONHXzW/LccbMdv8IfeLcOg30NN68XVyIaaLkdFoDbRXO69rnOFYM3q/q96E4ljKYgLPHgcBESZsfZ0jUt8gMyK12pwj6z4Y7DMj7ME+GOgxIdxUDGUfQe84s70sNrPt4FgzuPINNp9psZhrZTEztWwBJuSHX2uPN4HsZ5+af0MXuKigVkoFAdcDX3NtcyZOKcV1ObGsmxvDXwvm8eT7hXyzooW5oYM87LeKded7Y3AMLL4PDr4IazZDaCL0dULBX8yOw/P9+bLoHvjkGTixDZY94KpuCTG5tIa+DuhpNvXX9mpnEFePBnF7lRm1DoyzMjggYnSkGzsP7AkmCEMSRsMwKBqsk3Tk5P5uMzrvrDcBGxwHgRGuK5d4uIsufUyEq0sfn0Vrzc7CRv79zZMU1HVw++JEHr01h/CgcYK3pRz+azFc8RCs/3c48gpsexAeeBtSVp3vA+DnK8EvBL663bWdEeKzDIdvR+3ZgTs8Gu5qNDu9epqdO78GP70Nq68zgBPOuXaWHoYvNv+p75+XmazSx7SglGLNnGhWpUfysx1F/GxHEbsKG/jXjfO5KTf+7BeHp5hSxsEX4HMPm0UuobNg1hWf9QGw6F7Y/j1oOA3Rc1zaH+FBHI7xZxdN1ECPGTHaAsDH/9Pb1NoE63Bd9lM7xWpH67XjjX79Qk3QBkVDdJYZiQZEmJJeYISpwYbEQ0iiqct66Sh1OplxI+pz5Ve3879fO8KJ6nZuzo3j8Q3zibb7jb6g/qQZIS//Khx4Hlb/L7jusc/eaEcd/Hjuxb1WTB+d9XD4t+aIiX0dphQ2sjOs09RWw1PNn/4xORCbY64jnDujuxtNeHY2mBpu5/ClbszOsHroazv7c338TWjbAk2dtbP+nB1vTr728XeGjS1D2ONMPVhMO5My62MiPCmoAQaGHDy9s4Sn3iskyM/KP16byT0rkgnwdc6F3nIPFDjncP/9XvMDeCEv32VWN6avMSMTu3OEEpJgLqFJZseJmBpDg2bBQVe9ue4cczswChKXQsKiT+9QcgxB8Q7IewEK3jLlgdhc59SsYOeUrGATfhYfaCo0x4FpLh7dmWaxOcsK4/ws2QKdNVznjq3gWBOuvsFmZD3QY0bFAz3meDJDg6OzH0Z2hDlv+wW7+l9RuJHXB/WwwroOHn3jBHtLmogM8uWrV6dz38pk7I1H4Jl1EDMPvv4ZS8vHqjwIO35gzv7SUWOmEI2lLCasw1PPvgyPfuxxZhQlxqf1hf8kdzjMccIPv2xWm45XBrD6jRmdKojONqGduMTUcA/9GtoqTAlg4SZYcv/FlbMGeqGxwIR2wykzKg6ONtPJgmNM2SE4ZlrPNBBTS4L6HJ+UNfPTD4r48HQDoQE2HlidykNDW/BPXQFZN13aRnvbR3fitFeZHZUtZaOXrvpPv8cvdMw8zihTQ/QPM9cBzmv/UDP68rM7J987R3czsa5YfxJOvG4OKdtcYlaNJi2HpKXmOnSW6XdzqVkheniLWervFwrzb4e43E8HpW+wmdNbnQdVB0cv3U3mM9PXwtL7IetmM69XCDeRoD6Po5Wt/PcHRWzPryPYz4f7Vqbw1avTiAp2wQ9sf5cJ746asyftD+8k6m5y7qFvAT302dtSFjN538dv9DK8cMDq51z1NTy/1GqWu1tssORLkH3LxbW39riZqugYdC4mGBpdVDA0YOa5djePtrmn2dR0I2eb0WriErO0P3b++ac6am3COf91E9CNBYCClCtN6NYcgepDo4sXhksBNYfN6zLWmh272bdM7K8Trc1Zeyw+5q8eITyABPUFnKxp52c7injzWA1+PhbuXp7M19akEx/qhtKE1mbH1XAA9rY5d2p1mvLK8O3+LhNgg/3OxQPO24O956zWcoZrZ4MZfX7uO3DNP59/9oLWkPci/OU7oyUDNSbwlcWE/vCIP9A5myAg3NRjGwqcI9ZG816rrwlrv2Az02H4mAkD3c77XWabKashZyPMvc2UhYYNDUDdCaj8BCoPmL9OMq83ZYrQRJf+VwgxlSSoL1JxQye/+Gsxrx+qQim4Y2kSD63JICVyBuxFH+yDN79lVlXOWQ9feNqUVcbq74Y3vw1HXjYlgS/8ypRkJlpm0drUfasOmmOt1Bw2n28LHF2lNnw7It2Ec3DM5PVViGlIgnqCKlu6+eWHJbxyoILBIQe3LEjgb69MYUlyOGo614a1Nqsq394M4WmwaQtEZZrnmorh1b8xo9c134E1/0eOECjEFJKgvkT17b38alcJW/dX0NE3yLyEEO5flcqGRQn426ZxiJV9BK/eb46z8IWnTR369a+bYP7CM5B5nbtbKITXkaC+TF19g7x+uIqX9pRTUNdBWKCNu5bN4r6VKcyKmPqjbE2K1gp45V6zww7Mjr87X4SwZPe2SwgvJUE9SbTW7C9t5qW95bx9ohaH1qzLjuWB1alcmRE5/coiAz3w7nfNHOB1j8r0NCHcSILaBWrbevntx+W8/PEZmrr6mRMbzP1XpnL74kQCfWfcIVSEEC4mQe1CvQND/PloDc/vLuVEdTsh/j7ctXwWm1Ykkx4tS36FEBdHgnoKaK05WN7C83vKePt4LUMOzfLUcO5cNotbFsTLKFsI8ZkkqKdYfXsvv8+r4ncHKihp7CLYz4fbFsZz57JZLJoVNv1q2UIIl5OgdhOtNZ+UtfDqgQrePFpDz8AQqZGB3Jwbzy0L4smJD5HQFkIAEtQeoaN3gL8cq+HPR2vYU9zEkEOTFhXEzblx3JKbwNx4u4S2EF5MgtrDNHf1886JWt48WsOe4kYcGtKjgrh1QTy3LUwgM1YOjSmEt5Gg9mBNnX28faKWPx+pYV9pE1pDVqyd2xbGc+uCBFKjZsBxRoQQFyRBPU3Ud/Ty1rFa/nSkmgPlLQDkxIdwXU4s182NYX5CKBaLlEeEmIkkqKeh6tYe3jxawzsnask704JDQ4zdj3VzY1iXHcvq2VGjpxITQkx7EtTTXHNXPztO1fP+qTp2nm6ks28Qf5uFNXOiuXFeHOuyYwkNlPMzCjGdSVDPIP2DDvaXNrM9v5Z3TtRR296Lj0WxKiOS9fPjuD4nlhi7v7ubKYSYIAnqGcrh0BytauPt47W8fbyGsqZulIIlyeHckBPLDfPiSJOdkUJMCxLUXkBrzem6Tt4+Xsv2k7UcrzJnRc+MCeaGebFcnxNHbmIoVtkZKYRHkqD2QpUt3byXX8e7+XV8XNrMkEMTYLOSHW8nJz6EnIQQcuJDyI4LkZ2SQngACWov19rdz18LGjhS2Up+dTv5Ne109A4CYFGwcFYYGxcmcOvCBNecgV0IcUGXHdRKqTDgGWA+oIEva633nu/1EtSeTWtNZUsP+TXtnKhqY/vJek7WtGO1KK7OjOLzixK5YV6sHPFPiCk0GUH9IrBLa/2MUsoXCNRat57v9RLU08/pug5eP1TFG4erqWrtIcBmHVloc82cGJn+J4SLXVZQK6VCgcNAur7IOokE9fTlcGgOlLfw+uEq3jleS1NXP1aLYllKONfNjeXauTFkyAkRhJh0lxvUi4CngXxgIXAQ+IbWuut875GgnhmGHJojla28f7KO90/Wc6q2A4DUyECuzoxm9exIVqVHyWhbiElwuUG9DNgHrNZaf6yUegpo11p/75zXPQg8CJCcnLy0vLx8UhovPEdFczc7Cur54FQ9+0ub6e4fwqIgNzGUK2dHcdXsKJalhuPnI7NIhJioyw3qOGCf1jrVef9qYLPW+pbzvUdG1DNf/6CDwxWt7C5qZHdRI4cqWhlyaAJ9rayeHcW12TFckxVNfGiAu5sqxLTwWUF9wd36WutapVSFUipLa10ArMOUQYQX8/WxsCItghVpEXzz+jl09A7wcUkzOwrq+WtBA9vz6wCYGx/C2qxo1syJZnFyOL4+Fje3XIjp52JnfSzCTM/zBUqAB7TWLed7vYyovZvWmsL6TnacMmWSA+UtI6PtK9IiuCozmqtmRzEnNljOaiOEkyx4EW7V3jvAvuImPipq5KPCRkoazX7oaLsfV2dGsTYrhs9lRstOSeHVJKiFR6lq7WF3YSO7ihrZVdhAa/cAVotiaXI412RHc212DFmxcg5J4V0kqIXHGnJoDle0sONUAx+cqie/xhxMKi7En6syo7g6M4rVs6NkabuY8SSoxbRR29bLh6fr+fB0A7uLmmjrGQDMTsmrM80UwBVpEfjbZAqgmFkkqMW0NOTQHK9q4yNnieRgeQsDQxp/m4Ur0iJZMyeaNVnRpEcFSZlETHsS1GJG6O4f5OOSZj483cDOwgZKGsxOycSwANZkRXP17ChWZUQSFujr5pYKMXES1GJGqmjuZmdhAx8WNLCnuInOvkHU8ErJjNGVklImEdOBRwT1wMAAlZWV9Pb2TvrneSJ/f3+SkpKw2WTK2VQYGHJwtLKVjwqb2F3USN6ZFgYdGl8fC8tSwlk9O4orMyLJTQzFxyqLboTn8YigLi0txW63ExkZOePriVprmpqa6OjoIC0tzd3N8UpdfYPsL23mI+cS9+EDStn9fLgiPZLVsyO5anYUs2Nk0Y3wDJe1hHyy9Pb2kpqa6hU/FEopIiMjaWhocHdTvFaQnw9rs2NYmx0DQGNnH3uLm9hT3MjuoibeO2mWuMeH+nN1ZpTzaIBRRARJfVt4nik9hYc3hPQwb+rrdBAV7MdtCxO4bWECYOrbu4sa2VXYyDsn6nj1QOVIffsq507JpSnhcpYb4RG84lvY1NTEunXrAKitrcVqtRIdHQ3A/v378fW98CjqgQceYPPmzWRlZbm0rWJqzIoI5O4Vydy9Ipkhh+ZYVRu7Tjewq7CRp3eW8PO/FuNjUSycFcaq9EhWppvglhMBC3eYshr1yZMnmTt37qR/1kQ99thjBAcH8/DDD5/1uNYarTUWy+TtaPKUPouJ6eob5EB5C/tKmthX0sTRyjaGHBqbVbFoVhirMqJYlR7J4uQwmVEiJo1H1Kg9UVFRERs2bGDx4sUcOnSI7du38/jjj5OXl0dPTw933XUXjz76KABXXXUVP/3pT5k/fz5RUVE89NBDvPXWWwQGBvLGG28QExPj5t6IyRLk52MW08wxf3V19g1yoKyZvSVN7Ctu4qcfFPJf7xfi52NhaUo4q9IjWZURyYKkMDmMq3AJtwT14386QX51+6RuMychhH+5bd6E33fq1Cleeuklli0zv8ieeOIJIiIiGBwcZO3atdxxxx3k5OSc9Z62tjbWrFnDE088wbe+9S2ee+45Nm/ePCn9EJ4n2M+Ha7JiuCbL/DJu7x1gf4kJ7j3FTfzn9tOwHfxtFpalRLAyPYKV6RLcYvJ49YgaICMjYySkAbZs2cKzzz7L4OAg1dXV5OfnfyqoAwICuOmmmwBYunQpu3btmtI2C/cK8beZM7TnxALQ0tXP/rJmZ6mkmR+9exoYDe5VGZEyh1tcFrcE9aWMfF0lKCho5HZhYSFPPfUU+/fvJywsjPvuu2/cBTpjdz5arVYGBwenpK3CM4UH+XLjvDhunBcHjAb33mJT4/6/7xQAZmS+Ii2CKzPMzsm58SFYLTI7SFyY14+ox2pvb8dutxMSEkJNTQ3vvPMO69evd3ezxDRzbnA3dfaxr6SZPcWN7C1u4oNT9YAJ7sXJYSxPjWBZajiLZoXJdEAxLvlWjLFkyRJycnLIzs4mJSWF1atXu7tJYgaIDPbjlgXx3LIgHjCHcv24tIkDZS18UtbMT947jdbgY1HMSwzlirQIVqRGsDw1Qs56IwAvnJ43lbyxz2Li2noGyDvTwoGyZvaXNnOkoo3+IQdKQVasnZXpkSxPjWB5Wjgxdn93N1e4iEzPE8KDhQbYWJsVw1rnrJLegSEOV7Syv9QE9yufVPDCnjIA0qKCWJ4azvJUcwb45IhAWQXrBSSohfAw/jYrK52rIcEcGfBYVRuflDbzSVnzyJJ3gNgQP5amhLMkOZzFyeHMTwzBz0cW4cw0EtRCeDib1cKSZBPGX1uTgcOhKazvZH9ZM5+UNpN3poW/HKsFwNdqYV5iyMjrl6SEER8a4OYeiMslQS3ENGOxKLLi7GTF2fnSyhQA6jt6yStv5dCZFvLOtPCbfeU8+1EpYI4QaEbcYSxJCWdegoy6pxsJaiFmgBi7P+vnx7F+vpkS2D/o4GRNO3lnWsg700peeQtvHqsBwNfHwvyEEBYnh48EeEKYjLo9mQS1EDOQr4+FhbPCWDgrjAecs0zr2nudI24T3GNH3XEh/ixODmOR8z25iaEE+Uk8eAqv+Z9Yu3Ytmzdv5sYbbxx57Mknn6SgoIBf/OIX474nODiYzs7OqWqiEC4VG+LP+vnxrJ9v5nP3Dzo4VdtOXnkLhypayTvTwlvHTa3bomBOrJ2FSWEsSg5jQVIoWbF2WQLvJl4T1Js2bWLr1q1nBfXWrVv54Q9/6MZWCeE+vj4WFiSFsSApjL91PtbU2cfRyjYOV7RypLKVd/NreeVABWCOXTIvIZSFSWEsnGWuUyJleuBU8JqgvuOOO/jud79Lf38/vr6+lJWVUV1dzeLFi1m3bh0tLS0MDAzwgx/8gI0bN7q7uUK4RWSw31mnMNNac6a5myOVbRypaOVoZSsv7y/nud0OAEL8fchNCiU30Yy6cxNDSQoPkPCeZO4J6rc2Q+2xyd1mXC7c9MR5n46IiGDFihW89dZbbNy4ka1bt3LnnXcSEBDAtm3bCAkJobGxkZUrV7Jhwwb5ogmBOaVcSmQQKZFBbHCexmxwyMHpuk6OVrZytKqNY5VtPPtRCQNDZpVzeKCN+YmhzI0PYW68nbnxIWREB2OTssklu6igVkqVAR3AEDB4vmWOnm64/DEc1M8++yxaax555BF27tyJxWKhqqqKuro64uLi3N1cITySj9VCTkIIOQkh3O18rG9wiILaDo5WtnG0spX8mnZe2FNG/6AZedusitkxdnLiQ8hNDCE3KZSc+FA5tdlFmsiIeq3WunFSPvUzRr6utHHjRr75zW+Sl5dHd3c3S5cu5YUXXqChoYGDBw9is9lITU0d99CmQojz8/OxjtS7wcztHhhyUNrYxcmadvJr2jlZ08GHp+v5fZ5ZVWlRkBljZ35iKLmJIWTFhZAVZ5czwY/Da2rUYGZxrF27li9/+cts2rQJMGdriYmJwWazsWPHDsrLy93cSiFmBpvVwpxYO3Ni7WxclAiYmndtey/HKts4XtXGsao2PjzdMBLeANF2P7Kc78uKC2Z2jJ3ZMcGEBnjvkQQvNqg18K5SSgO/1Fo/7cI2udSmTZu4/fbb2bp1KwD33nsvt912G7m5uSxbtozs7Gw3t1CImUspRXxoAPGhAdzgPF631pqGjj5O1XZwuq6DgtoOCuo6eHl/Ob0DjpH3xtj9yIwNJjPGTkZMMHNigpkTayfcC0bgF3WYU6VUota6SikVA2wH/qfWeuc5r3kQeBAgOTl56bkjU2885Kc39lmIyTLk0FQ0d1NU30lhfSdF9Z0U1XdQVN9JV//QyOvOHYEPj+Kn24Kdyz7Mqda6ynldr5TaBqwAdp7zmqeBp8Ecj/qyWiyE8HpWiyI1KojUqKCR81OCGYHXtPVyuq7Deenk9Dgj8JTIQLJi7WTH2cmON/XvlIjAablo54JBrZQKAixa6w7n7RuA77u8ZUIIMQ6lFAlhASSEBYycGR5GR+AFzvLJqdp2TtV28N7JOhzOoaPNqkiLCiLTWfcevqREBnr0adAupmWxwDbnvGIf4GWt9dsubZUQQkzQ2BH48PkqwZyIobCuk4K6jpHyyfHqNv5yvIaxld+oYD+SIwJIjggkOSKQWRGBpEUFkRYVRESQr1vXVlwwqLXWJcDCyfgwrbXXLCRxxSnOhBAT52+zmtWTSaFnPd47MERJQxfFDZ2cae6mormbM83dHChv4Y9HqkdG4WDOwpMWFUR6dBAZ0cGkRwWRHm1G4v42188Fn7Kxvr+/P01NTURGRs74sNZa09TUhL+/nN9OCE/lb7OOLNw518CQg6qWHkobuyhp7KKkoZOShi72FDXxh7yqkddZFCSFB44EeEZ0MJtWzJr0jJuyoE5KSqKyspKGhoap+ki38vf3Jykpyd3NEEJcApvVMlJGWXvOc119g5Q2mpF4cYMJ8eKGLvaVNBEe6Ms9VyRPenumLKhtNhtpaWlT9XFCCOESQX4+zE8MZX7i2aUUh0PT3N3vks+cfvNUhBDCA1ksiqhgP9ds2yVbFUIIMWkkqIUQwsNd1BLyCW9UqQbgUo9uFAVMzlH6phfpt3eRfnuXi+l3itY6erwnXBLUl0MpdWC6Hu/6cki/vYv027tcbr+l9CGEEB5OgloIITycJwb1tD3W9WWSfnsX6bd3uax+e1yNWgghxNk8cUQthBBiDI8JaqXUeqVUgVKqSCm12d3tcSWl1HNKqXql1PExj0UopbYrpQqd1+HubONkU0rNUkrtUErlK6VOKKW+4Xx8RvcbQCnlr5Tar5Q64uz7487H05RSHzu/868opWbcOaWUUlal1CGl1J+d92d8nwGUUmVKqWNKqcNKqQPOxy75u+4RQa2UsgI/A24CcoBNSqkc97bKpV4A1p/z2Gbgfa11JvC+8/5MMgh8W2udA6wE/sH5fzzT+w3QB1yrtV4ILALWK6VWAv8B/ERrPRtoAb7ixja6yjeAk2Pue0Ofh63VWi8aMy3vkr/rHhHUmFN7FWmtS7TW/cBWYKOb2+QyzvNNNp/z8EbgReftF4HPT2mjXExrXaO1znPe7sD88CYyw/sNoI1O512b86KBa4HXnI/PuL4rpZKAW4BnnPcVM7zPF3DJ33VPCepEoGLM/UrnY94kVmtd47xdizmzzoyklEoFFgMf4yX9dpYADgP1mBNEFwOtWutB50tm4nf+SeA7wPCJDCOZ+X0epoF3lVIHnSf+hsv4rnvuScK8mNZaK6Vm5HQcpVQw8Hvgn7TW7WMPsD6T+621HgIWKaXCgG1Atpub5FJKqVuBeq31QaXUNe5ujxtcpbWuUkrFANuVUqfGPjnR77qnjKirgFlj7ic5H/MmdUqpeADndb2b2zPplFI2TEj/Vmv9B+fDM77fY2mtW4EdwCogTCk1PFiaad/51cAGpVQZppR5LfAUM7vPI7TWVc7reswv5hVcxnfdU4L6EyDTuUfYF7gb+KOb2zTV/gjc77x9PzM9XpYAAAEPSURBVPCGG9sy6Zz1yWeBk1rrH495akb3G0ApFe0cSaOUCgCux9TodwB3OF82o/qutf5nrXWS1joV8/P8gdb6XmZwn4cppYKUUvbh28ANwHEu47vuMQtelFI3Y2paVuA5rfW/ublJLqOU2gJcgzmiVh3wL8DrwKtAMubIg3dqrc/d4ThtKaWuAnYBxxitWT6CqVPP2H4DKKUWYHYeWTGDo1e11t9XSqVjRpsRwCHgPq11n/ta6hrO0sfDWutbvaHPzj5uc971AV7WWv+bUiqSS/yue0xQCyGEGJ+nlD6EEEKchwS1EEJ4OAlqIYTwcBLUQgjh4SSohRDCw0lQCyGEh5OgFkIIDydBLYQQHu7/A2YFNmgxj9PWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.plotLearningCurve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
