{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "## Load libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_glove_file = r\"/Users/anshuman/UMass/CS 682/Project/glove.6B/glove.6b.300d.txt\" ## glove text file that you download from internet, 6b words 300d,\n",
    "new_glove_file =r\"glove6b300d.bin\"\n",
    "movie_data_file =r\"ml-1m/movies.dat\" #movie data file\n",
    "user_data_file =r\"ml-1m/users.dat\" #user data file\n",
    "rating_data_file = r\"ml-1m/ratings.dat\" #rating data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this only once ever, if you have skip this and go to the next cell\n",
    "## Insert your glove location below.\n",
    "## This is done to convert .txt glove to binary format for easier loading.\n",
    "glove_file = datapath(og_glove_file)\n",
    "tmp_file = get_tmpfile(\"glove.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "model.save_word2vec_format(new_glove_file,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshuman/anaconda3/envs/cs682/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    " ##load binary glove model\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format(new_glove_file,binary=True)\n",
    "##get vocabulary\n",
    "vocab = list(glove_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the dataset into dataframes\n",
    "\n",
    "movieData = pd.read_csv(movie_data_file,sep=\"::\",names=[\"MovieID\",\"Name\",\"Genres\"],engine='python')\n",
    "userData = pd.read_csv(user_data_file,sep=\"::\",names=[\"UserID\",\"Gender\",\"Age\",\"Occupation\",\"Zipcode\"],engine='python')\n",
    "ratingData = pd.read_csv(rating_data_file,sep=\"::\",names=[\"UserID\",\"MovieID\",\"Rating\",\"Timestamp\"],engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## movie data preprocessing  - getting a dictionary of movie name and id pairs,\n",
    "##creating genrelist, and reseting movieData dataframe\n",
    "\n",
    "movieNameData = movieData[\"Name\"].unique()\n",
    "movieNameDict = {movieNameData[i]:i for i in range(0,movieNameData.shape[0])}\n",
    "genreList = [\"Action\",\"Adventure\",\"Animation\",\"Children's\",\"Comedy\",\"Crime\",\"Documentary\",\"Drama\",\"Fantasy\",\"Film-Noir\",\n",
    "              \"Horror\",\"Musical\",\"Mystery\",\"Romance\",\"Sci-Fi\",\"Thriller\",\"War\",\"Western\"]\n",
    "genres = movieData[\"Genres\"].apply(lambda x : x.split(\"|\"))\n",
    "movieData.reset_index(inplace=True,drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 3)\n",
      "Movie Embeddings (3953, 318)\n"
     ]
    }
   ],
   "source": [
    "## Create movie data vector based on embeddings or without.\n",
    "## genre data is on hot encoded since each film could have one or more genre. Array size of the one hot encoding is 18 \n",
    "## structure : movieid:genredata for no  word2vec embedding\n",
    "## structure : movie_name_embedding:genredata for no  word2vec embedding\n",
    "\n",
    "### In MovieEncodingEmbedding(), case-sensitivity is removed and also all characters which are not alpha numeric are removed.\n",
    "### If the word isn't part of glove vocab, a 0 vector is used in its stead.\n",
    "# def MovieEncodingNoEmbedding(movieid):\n",
    "    \n",
    "#     movieDataIDRow = movieData.loc[movieData[\"MovieID\"]==movieid]\n",
    "#     movieDataMovieID = movieNameDict[movieDataIDRow.Name.values[0]]\n",
    "#     movieDataGenreEncoding = [1 if i in movieDataIDRow.Genres.values[0].split(\"|\") else 0  for i in genreList]\n",
    "#     data = [movieDataMovieID]+movieDataGenreEncoding\n",
    "#     return np.array(data)\n",
    "\n",
    "# def MovieEncodingEmbedding(movieid):\n",
    "#     movieDataIDRow = movieData.loc[movieData[\"MovieID\"]==movieid]\n",
    "# #     print(\"Movie name\", movieDataIDRow.Name)\n",
    "#     movieDataMovieID = ''.join([char for char in movieDataIDRow.Name.values[0].lower() if (char.isalpha() or char==\" \")]).split(\" \")\n",
    "#     movieNameEmbedding = np.mean([glove_model[word] if (word in vocab) else np.zeros((1,300)) for word in movieDataMovieID],axis=0)                            \n",
    "#     movieDataGenreEncoding = np.array([1 if i in movieDataIDRow.Genres.values[0].split(\"|\") else 0  for i in genreList])\n",
    "#     data = np.concatenate((movieNameEmbedding, movieDataGenreEncoding.reshape(1,movieDataGenreEncoding.shape[0])), axis=None)\n",
    "#     return data.reshape(1,data.shape[0])\n",
    "\n",
    "from data_preparation import MovieEncodingEmbedding\n",
    "\n",
    "print(movieData.shape)\n",
    "numRows = movieData.shape[0]\n",
    "# print(movieData, movieData.loc[0][0], movieData.loc[0][1], movieData.loc[0][2])\n",
    "movie_embeddings = np.zeros((3953, 318))\n",
    "for rowNum in range(numRows):\n",
    "    movieID = movieData.loc[rowNum]['MovieID']\n",
    "#     print(rowNum, movieID)\n",
    "    movie_embeddings[movieID] = MovieEncodingEmbedding(movieID, movieData, genreList, glove_model, vocab)\n",
    "\n",
    "print(\"Movie Embeddings\", movie_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "from data_preparation import UserSequences\n",
    "\n",
    "## get user Sequences\n",
    "userSequences = UserSequences(ratingData,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5421\n",
      "27\n",
      "(3873,)\n"
     ]
    }
   ],
   "source": [
    "print(len(userSequences))\n",
    "print(len(userSequences[0]))\n",
    "print(userSequences[99][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get user-movie encodings for the data with embedding flag false and write it to file.\n",
    "## In every file each line will contain\n",
    "##userid::gender::userDataAge::userDataOccupation::userDataZipCode::movieid::(one hot genre encoding of len 18[each separated by ::]):rating \n",
    "# data = UserMoviePairEncodingBatch(userSequences,False)\n",
    "\n",
    "# with open(\"usermovie.dat\",\"w\") as f:\n",
    "#     for i in range(len(data)):\n",
    "#         for j in range(data[i].shape[0]):\n",
    "#             f.write(\"::\".join(data[i][j]))\n",
    "#             f.write(\"\\n\")\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get user-movie encodings for the data with embedding flag false and write it to file.\n",
    "## In every file each line will contain\n",
    "##userid::gender::userDataAge::userDataOccupation::userDataZipCode::(movieid_embedding[len 300, separated by ::])::(one hot genre encoding of len 18[each separated by ::]):rating \n",
    "\n",
    "# data = UserMoviePairEncodingBatch(userSequences,True)\n",
    "\n",
    "# with open(\"usermoviembeddings.dat\",\"w\") as f:\n",
    "#     for i in range(len(data)):\n",
    "#         for j in range(data[i].shape[0]):\n",
    "#             f.write(\"::\".join(data[i][j]))\n",
    "#             f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3468, 27) (3468, 26, 318) (3468, 26)\n",
      "val (868, 27) (868, 26, 318) (868, 26)\n",
      "test (1085, 27) (1085, 26, 318) (1085, 26)\n"
     ]
    }
   ],
   "source": [
    "# np.array(data).shape\n",
    "# print (\"data\", len(data), data[0].shape)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def fix(data):\n",
    "    newData = np.empty(data.shape, dtype=int)\n",
    "    for i, userSeq in enumerate(data):\n",
    "        for j, tup in enumerate(userSeq):\n",
    "            newData[i][j] = tup[0]\n",
    "    return newData\n",
    "\n",
    "data = fix(np.array(userSequences))\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "train_inp = movie_embeddings[train_data[:, :-1]]\n",
    "train_tgt = train_data[:, 1:]\n",
    "\n",
    "test_inp = movie_embeddings[test_data[:, :-1]]\n",
    "test_tgt = test_data[:, 1:]\n",
    "\n",
    "val_inp = movie_embeddings[val_data[:, :-1]]\n",
    "val_tgt = val_data[:, 1:]\n",
    "\n",
    "print (\"train\", train_data.shape, train_inp.shape, train_tgt.shape)\n",
    "print (\"val\", val_data.shape, val_inp.shape, val_tgt.shape)\n",
    "print (\"test\", test_data.shape, test_inp.shape, test_tgt.shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train torch.Size([3468, 26, 318]) torch.Size([3468, 26])\n",
      "Val torch.Size([868, 26, 318]) torch.Size([868, 26])\n",
      "Test torch.Size([1085, 26, 318]) torch.Size([1085, 26])\n"
     ]
    }
   ],
   "source": [
    "# train_data = batchify(torch.Tensor(train_data), 10)\n",
    "# val_data = batchify(torch.Tensor(val_data), 10)\n",
    "# test_data = batchify(torch.Tensor(test_data), 10)\n",
    "\n",
    "train_data_final = torch.Tensor(train_inp)\n",
    "val_data_final = torch.Tensor(val_inp)\n",
    "test_data_final = torch.Tensor(test_inp)\n",
    "\n",
    "train_tgt_final = torch.Tensor(train_tgt).long()\n",
    "val_tgt_final = torch.Tensor(val_tgt).long()\n",
    "test_tgt_final = torch.Tensor(test_tgt).long()\n",
    "\n",
    "print(\"Train\", train_data_final.shape, train_tgt_final.shape)\n",
    "print(\"Val\", val_data_final.shape, val_tgt_final.shape)\n",
    "print(\"Test\", test_data_final.shape, test_tgt_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |\n",
      "  100/  347 batches | lr 1.00 | loss  7.45\n",
      "  200/  347 batches | lr 1.00 | loss  7.27\n",
      "  300/  347 batches | lr 1.00 | loss  7.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 20.29s | val loss  7.04 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshuman/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/  347 batches | lr 1.00 | loss  7.02\n",
      "  200/  347 batches | lr 1.00 | loss  7.01\n",
      "  300/  347 batches | lr 1.00 | loss  7.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 20.40s | val loss  7.02 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |\n",
      "  100/  347 batches | lr 1.00 | loss  7.00\n",
      "  200/  347 batches | lr 1.00 | loss  7.00\n",
      "  300/  347 batches | lr 1.00 | loss  6.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 20.91s | val loss  7.02 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.99\n",
      "  200/  347 batches | lr 1.00 | loss  6.99\n",
      "  300/  347 batches | lr 1.00 | loss  6.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 20.98s | val loss  7.00 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.97\n",
      "  200/  347 batches | lr 1.00 | loss  6.96\n",
      "  300/  347 batches | lr 1.00 | loss  6.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 21.05s | val loss  6.96 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.92\n",
      "  200/  347 batches | lr 1.00 | loss  6.91\n",
      "  300/  347 batches | lr 1.00 | loss  6.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 20.88s | val loss  6.91 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.85\n",
      "  200/  347 batches | lr 1.00 | loss  6.84\n",
      "  300/  347 batches | lr 1.00 | loss  6.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 20.73s | val loss  6.85 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.77\n",
      "  200/  347 batches | lr 1.00 | loss  6.76\n",
      "  300/  347 batches | lr 1.00 | loss  6.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 20.54s | val loss  6.80 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.70\n",
      "  200/  347 batches | lr 1.00 | loss  6.69\n",
      "  300/  347 batches | lr 1.00 | loss  6.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 20.48s | val loss  6.83 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.63\n",
      "  200/  347 batches | lr 1.00 | loss  6.63\n",
      "  300/  347 batches | lr 1.00 | loss  6.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 20.60s | val loss  6.79 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.56\n",
      "  200/  347 batches | lr 1.00 | loss  6.56\n",
      "  300/  347 batches | lr 1.00 | loss  6.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 20.37s | val loss  6.76 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.50\n",
      "  200/  347 batches | lr 1.00 | loss  6.50\n",
      "  300/  347 batches | lr 1.00 | loss  6.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 20.52s | val loss  6.73 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.45\n",
      "  200/  347 batches | lr 1.00 | loss  6.45\n",
      "  300/  347 batches | lr 1.00 | loss  6.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 20.50s | val loss  6.70 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.40\n",
      "  200/  347 batches | lr 1.00 | loss  6.40\n",
      "  300/  347 batches | lr 1.00 | loss  6.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 20.41s | val loss  6.68 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.35\n",
      "  200/  347 batches | lr 1.00 | loss  6.35\n",
      "  300/  347 batches | lr 1.00 | loss  6.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 20.32s | val loss  6.66 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.30\n",
      "  200/  347 batches | lr 1.00 | loss  6.31\n",
      "  300/  347 batches | lr 1.00 | loss  6.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 20.53s | val loss  6.64 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.26\n",
      "  200/  347 batches | lr 1.00 | loss  6.26\n",
      "  300/  347 batches | lr 1.00 | loss  6.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 20.46s | val loss  6.63 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.21\n",
      "  200/  347 batches | lr 1.00 | loss  6.22\n",
      "  300/  347 batches | lr 1.00 | loss  6.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 20.44s | val loss  6.62 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.17\n",
      "  200/  347 batches | lr 1.00 | loss  6.18\n",
      "  300/  347 batches | lr 1.00 | loss  6.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 20.34s | val loss  6.61 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.13\n",
      "  200/  347 batches | lr 1.00 | loss  6.15\n",
      "  300/  347 batches | lr 1.00 | loss  6.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 20.45s | val loss  6.60 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.10\n",
      "  200/  347 batches | lr 1.00 | loss  6.11\n",
      "  300/  347 batches | lr 1.00 | loss  6.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 20.38s | val loss  6.59 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.06\n",
      "  200/  347 batches | lr 1.00 | loss  6.07\n",
      "  300/  347 batches | lr 1.00 | loss  6.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 20.41s | val loss  6.59 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |\n",
      "  100/  347 batches | lr 1.00 | loss  6.02\n",
      "  200/  347 batches | lr 1.00 | loss  6.04\n",
      "  300/  347 batches | lr 1.00 | loss  6.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 20.45s | val loss  6.59 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/  347 batches | lr 1.00 | loss  5.99\n",
      "  200/  347 batches | lr 1.00 | loss  6.00\n",
      "  300/  347 batches | lr 1.00 | loss  5.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 20.70s | val loss  6.59 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.95\n",
      "  200/  347 batches | lr 1.00 | loss  5.96\n",
      "  300/  347 batches | lr 1.00 | loss  5.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 20.59s | val loss  6.59 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.91\n",
      "  200/  347 batches | lr 1.00 | loss  5.93\n",
      "  300/  347 batches | lr 1.00 | loss  5.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 20.47s | val loss  6.59 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.87\n",
      "  200/  347 batches | lr 1.00 | loss  5.89\n",
      "  300/  347 batches | lr 1.00 | loss  5.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 20.48s | val loss  6.59 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.84\n",
      "  200/  347 batches | lr 1.00 | loss  5.85\n",
      "  300/  347 batches | lr 1.00 | loss  5.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 20.48s | val loss  6.59 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.80\n",
      "  200/  347 batches | lr 1.00 | loss  5.82\n",
      "  300/  347 batches | lr 1.00 | loss  5.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 20.48s | val loss  6.59 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.76\n",
      "  200/  347 batches | lr 1.00 | loss  5.78\n",
      "  300/  347 batches | lr 1.00 | loss  5.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 20.65s | val loss  6.56 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.73\n",
      "  200/  347 batches | lr 1.00 | loss  5.74\n",
      "  300/  347 batches | lr 1.00 | loss  5.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 20.48s | val loss  6.54 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.69\n",
      "  200/  347 batches | lr 1.00 | loss  5.71\n",
      "  300/  347 batches | lr 1.00 | loss  5.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 20.36s | val loss  6.55 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.65\n",
      "  200/  347 batches | lr 1.00 | loss  5.67\n",
      "  300/  347 batches | lr 1.00 | loss  5.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 20.35s | val loss  6.56 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.62\n",
      "  200/  347 batches | lr 1.00 | loss  5.63\n",
      "  300/  347 batches | lr 1.00 | loss  5.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 20.64s | val loss  6.57 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.58\n",
      "  200/  347 batches | lr 1.00 | loss  5.59\n",
      "  300/  347 batches | lr 1.00 | loss  5.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 20.39s | val loss  6.58 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.54\n",
      "  200/  347 batches | lr 1.00 | loss  5.56\n",
      "  300/  347 batches | lr 1.00 | loss  5.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 20.43s | val loss  6.59 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.50\n",
      "  200/  347 batches | lr 1.00 | loss  5.52\n",
      "  300/  347 batches | lr 1.00 | loss  5.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 20.39s | val loss  6.60 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.46\n",
      "  200/  347 batches | lr 1.00 | loss  5.48\n",
      "  300/  347 batches | lr 1.00 | loss  5.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 20.44s | val loss  6.62 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.42\n",
      "  200/  347 batches | lr 1.00 | loss  5.44\n",
      "  300/  347 batches | lr 1.00 | loss  5.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 20.65s | val loss  6.63 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.38\n",
      "  200/  347 batches | lr 1.00 | loss  5.40\n",
      "  300/  347 batches | lr 1.00 | loss  5.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 20.39s | val loss  6.64 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.34\n",
      "  200/  347 batches | lr 1.00 | loss  5.36\n",
      "  300/  347 batches | lr 1.00 | loss  5.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 20.47s | val loss  6.66 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.30\n",
      "  200/  347 batches | lr 1.00 | loss  5.32\n",
      "  300/  347 batches | lr 1.00 | loss  5.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 20.57s | val loss  6.67 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.26\n",
      "  200/  347 batches | lr 1.00 | loss  5.28\n",
      "  300/  347 batches | lr 1.00 | loss  5.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 20.55s | val loss  6.69 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.21\n",
      "  200/  347 batches | lr 1.00 | loss  5.24\n",
      "  300/  347 batches | lr 1.00 | loss  5.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 20.48s | val loss  6.70 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.17\n",
      "  200/  347 batches | lr 1.00 | loss  5.19\n",
      "  300/  347 batches | lr 1.00 | loss  5.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 20.39s | val loss  6.72 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/  347 batches | lr 1.00 | loss  5.13\n",
      "  200/  347 batches | lr 1.00 | loss  5.15\n",
      "  300/  347 batches | lr 1.00 | loss  5.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 20.53s | val loss  6.74 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.09\n",
      "  200/  347 batches | lr 1.00 | loss  5.11\n",
      "  300/  347 batches | lr 1.00 | loss  5.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 20.51s | val loss  6.75 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.05\n",
      "  200/  347 batches | lr 1.00 | loss  5.07\n",
      "  300/  347 batches | lr 1.00 | loss  5.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 20.48s | val loss  6.77 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |\n",
      "  100/  347 batches | lr 1.00 | loss  5.00\n",
      "  200/  347 batches | lr 1.00 | loss  5.03\n",
      "  300/  347 batches | lr 1.00 | loss  5.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 20.36s | val loss  6.78 \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |\n",
      "  100/  347 batches | lr 1.00 | loss  4.96\n",
      "  200/  347 batches | lr 1.00 | loss  4.99\n",
      "  300/  347 batches | lr 1.00 | loss  4.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 20.79s | val loss  6.80 \n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from model import RNNModel\n",
    "from learner import Learner\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "_, temporal_batch_size, input_size = train_data_final.shape\n",
    "ntokens = 3953\n",
    "nlayers = 2\n",
    "hidden_size = 200\n",
    "\n",
    "model = RNNModel('gru', ntokens, input_size, hidden_size, nlayers, temporal_batch_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learner = Learner(model, criterion)\n",
    "\n",
    "best_val_loss = None\n",
    "num_epochs = 50\n",
    "\n",
    "# Loop over epochs.\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print (\"| epoch {:3d} |\".format(epoch))\n",
    "    epoch_start_time = time.time()\n",
    "    learner.train(train_data_final, train_tgt_final, ntokens, lr=1, batch_size=10)\n",
    "    val_loss = learner.evaluate(val_data_final, val_tgt_final, ntokens)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | val loss {:5.2f} '\n",
    "          .format(epoch, (time.time() - epoch_start_time),\n",
    "                                       val_loss))\n",
    "    print('-' * 89)\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(\"model.pt\", 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "#     else if lr > 1e-3:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "#         lr /= 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.plotLearningCurve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
